{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "27cda2713a5cdf633631f7bb7870b498235df809",
    "collapsed": false
   },
   "source": [
    "This notebook is based on Google codelab's tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "17c3825a-3225-436f-8ed7-a9bf52f29eb7",
    "_execution_state": "idle",
    "_uuid": "f4e98ca18e093cff0a928f582f25d87af584aadb",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "93540a420a51c00a9dd8aeb66705c6b0dfd8067f",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "3e6a6c015ef4c34ba456de359d58c2fc0bf0d45a",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data(42000,785)\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read training data from CSV file \n",
    "data = pd.read_csv('./input/train.csv')\n",
    "\n",
    "print('data({0[0]},{0[1]})'.format(data.shape))\n",
    "print (data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "bc8e150ac53bc3e698d63c74dd6088fb1bf92056",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 28, 28, 1)\n",
      "(42000, 10)\n",
      "420\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "images = data.iloc[:,1:]\n",
    "# convert from [0:255] => [0.0:1.0]\n",
    "def image_process(df):\n",
    "    df = df.values.astype(np.float)\n",
    "    df = np.multiply(df, 1.0 / 255.0)\n",
    "    df = df.reshape(-1,28,28,1)   \n",
    "    return df\n",
    "images = image_process(images)\n",
    "print images.shape\n",
    "\n",
    "labels = data.iloc[:,0]\n",
    "labels_count = np.unique(labels).shape[0]\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "labels = dense_to_one_hot(labels, labels_count)\n",
    "labels = labels.astype(np.uint8)\n",
    "print labels.shape\n",
    "\n",
    "X_input = np.split(images,len(data)/100)\n",
    "y_input = np.split(labels,len(data)/100)\n",
    "print len(X_input)\n",
    "print len(y_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "bf2e950f8c874ca1fea6144cac4053356f274fd9",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "# correct answers will go here\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# weights W[784, 10]   784=28*28\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "# biases b[10]\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# flatten the images into a single line of pixels\n",
    "# -1 in the shape definition means \"the only possible dimension that will preserve the number of elements\"\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "\n",
    "# The model\n",
    "Y = tf.nn.softmax(tf.matmul(XX, W) + b)\n",
    "\n",
    "# loss function: cross-entropy = - sum( Y_i * log(Yi) )\n",
    "#                           Y: the computed output vector\n",
    "#                           Y_: the desired output vector\n",
    "\n",
    "# cross-entropy\n",
    "# log takes the log of each element, * multiplies the tensors element by element\n",
    "# reduce_mean will add all the components in the tensor\n",
    "# so here we end up with the total cross-entropy for all images in the batch\n",
    "cross_entropy = -tf.reduce_mean(Y_ * tf.log(Y)) * 1000.0  # normalized for batches of 100 images,\n",
    "                                                          # *10 because  \"mean\" included an unwanted division by 10\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "predict = tf.argmax(Y,1)\n",
    "\n",
    "# training, learning rate = 0.005\n",
    "train_step = tf.train.GradientDescentOptimizer(0.005).minimize(cross_entropy)\n",
    "\n",
    "allweights = tf.reshape(W, [-1])\n",
    "allbiases = tf.reshape(b, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "cc479d579b62ee66a5af11fe33cd1f9c6d3e587d",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "3b40fd9d767c4320f19443acb6a745bd71e878f1",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can call this function in a loop to train the model, 100 images at a time\n",
    "accuracy_list = pd.Series()\n",
    "def training_step(i,batch_X, batch_Y,accuracy_list):\n",
    "\n",
    "    # training on batches of 100 images with 100 labels\n",
    "    #batch_X = X_input.ix[i*100:i*100+100].values\n",
    "    #batch_Y  = y_input.ix[i*100:i*100+100].values\n",
    "\n",
    "    # compute training values for visualisation\n",
    "    if i % 20 == 0:\n",
    "        a, c, w, b ,co= sess.run([accuracy, cross_entropy, allweights, allbiases,correct_prediction], feed_dict={X: batch_X, Y_: batch_Y})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "    #accuracy_list.append(a)\n",
    "        accuracy_list = accuracy_list.set_value(i, str(a))\n",
    "    #print str(co)\n",
    "\n",
    "    # the backpropagation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.93 loss: 24.8326\n",
      "20: accuracy:0.93 loss: 30.1758\n",
      "40: accuracy:0.94 loss: 24.2318\n",
      "60: accuracy:0.91 loss: 21.5755\n",
      "80: accuracy:0.93 loss: 28.7433\n",
      "100: accuracy:0.97 loss: 18.1682\n",
      "120: accuracy:0.95 loss: 19.0356\n",
      "140: accuracy:0.91 loss: 31.3063\n",
      "160: accuracy:0.92 loss: 34.9279\n",
      "180: accuracy:0.91 loss: 23.7352\n",
      "200: accuracy:0.92 loss: 31.4308\n",
      "220: accuracy:0.92 loss: 32.0196\n",
      "240: accuracy:0.92 loss: 26.3124\n",
      "260: accuracy:0.92 loss: 36.2982\n",
      "280: accuracy:0.91 loss: 31.0324\n",
      "300: accuracy:0.91 loss: 30.1672\n",
      "320: accuracy:0.87 loss: 35.4409\n",
      "340: accuracy:0.92 loss: 29.233\n",
      "360: accuracy:0.92 loss: 20.3925\n",
      "380: accuracy:0.87 loss: 32.3913\n",
      "400: accuracy:0.88 loss: 42.5342\n"
     ]
    }
   ],
   "source": [
    "for i, (batch_X, batch_Y) in enumerate(zip(X_input, y_input)):\n",
    "    training_step(i,batch_X, batch_Y,accuracy_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11836cb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check final accuracy on validation set  \n",
    "VALIDATION_SIZE = 2000\n",
    "if(VALIDATION_SIZE):\n",
    "    #validation_accuracy = accuracy.eval(image)\n",
    "    #print('training_accruacy => %.4f'%accuracy_list)\n",
    "    #print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot( accuracy_list,'-b', label='Training')\n",
    "    #plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.7)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.93\n",
       "20     0.93\n",
       "40     0.94\n",
       "60     0.91\n",
       "80     0.93\n",
       "100    0.97\n",
       "120    0.95\n",
       "140    0.91\n",
       "160    0.92\n",
       "180    0.91\n",
       "200    0.92\n",
       "220    0.92\n",
       "240    0.92\n",
       "260    0.92\n",
       "280    0.91\n",
       "300    0.91\n",
       "320    0.87\n",
       "340    0.92\n",
       "360    0.92\n",
       "380    0.87\n",
       "400    0.88\n",
       "dtype: object"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Shape [-1,28,28,1] has negative dimensions\n\t [[Node: Placeholder_16 = Placeholder[dtype=DT_FLOAT, shape=[?,28,28,1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'Placeholder_16', defined at:\n  File \"/Users/crisliu/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-129-7f0605648f66>\", line 3, in <module>\n    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1530, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1954, in _placeholder\n    name=name)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Shape [-1,28,28,1] has negative dimensions\n\t [[Node: Placeholder_16 = Placeholder[dtype=DT_FLOAT, shape=[?,28,28,1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-cc19ffd2fb9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \"\"\"\n\u001b[0;32m--> 606\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3926\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3927\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3928\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape [-1,28,28,1] has negative dimensions\n\t [[Node: Placeholder_16 = Placeholder[dtype=DT_FLOAT, shape=[?,28,28,1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'Placeholder_16', defined at:\n  File \"/Users/crisliu/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-129-7f0605648f66>\", line 3, in <module>\n    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1530, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1954, in _placeholder\n    name=name)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/crisliu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Shape [-1,28,28,1] has negative dimensions\n\t [[Node: Placeholder_16 = Placeholder[dtype=DT_FLOAT, shape=[?,28,28,1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "accuracy.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2(28000,784)\n",
      "(28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('./input/test.csv')\n",
    "print('data2({0[0]},{0[1]})'.format(data2.shape))\n",
    "test_image = image_process(data2)\n",
    "print test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_labels = sess.run(predict, feed_dict={X: test_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3313\n",
       "4    2961\n",
       "3    2934\n",
       "9    2873\n",
       "0    2785\n",
       "8    2741\n",
       "7    2734\n",
       "6    2716\n",
       "2    2589\n",
       "5    2354\n",
       "dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predicted_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2],\n",
       "       [    2,     0],\n",
       "       [    3,     9],\n",
       "       ..., \n",
       "       [27998,     3],\n",
       "       [27999,     9],\n",
       "       [28000,     2]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.c_[range(1,len(test_image)+1),predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "00fbcd9b1bd5734e321c0b42f5fd24e9ac990acc",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "batch_X, batch_Y = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training_step(i, update_test_data, update_train_data):\n",
    "\n",
    "    # training on batches of 100 images with 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "\n",
    "    # compute training values for visualisation\n",
    "    #if update_train_data:\n",
    "    a, c,  w, b = sess.run([accuracy, cross_entropy,  allweights, allbiases], feed_dict={X: batch_X, Y_: batch_Y})\n",
    "        #datavis.append_training_curves_data(i, a, c)\n",
    "        #datavis.append_data_histograms(i, w, b)\n",
    "        #datavis.update_image1(im)\n",
    "    print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "\n",
    "    # compute test values for visualisation\n",
    "    #if update_test_data:\n",
    "        #a, c = sess.run([accuracy, cross_entropy], feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        #datavis.append_test_curves_data(i, a, c)\n",
    "        #datavis.update_image2(im)\n",
    "        #print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "\n",
    "    # the backpropagation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.07 loss: 230.259\n",
      "1: accuracy:0.18 loss: 197.406\n",
      "2: accuracy:0.39 loss: 197.469\n",
      "3: accuracy:0.49 loss: 161.231\n",
      "4: accuracy:0.49 loss: 146.902\n",
      "5: accuracy:0.71 loss: 121.839\n",
      "6: accuracy:0.66 loss: 105.253\n",
      "7: accuracy:0.67 loss: 109.099\n",
      "8: accuracy:0.58 loss: 122.325\n",
      "9: accuracy:0.61 loss: 103.686\n",
      "10: accuracy:0.76 loss: 78.3923\n",
      "11: accuracy:0.74 loss: 85.1512\n",
      "12: accuracy:0.78 loss: 83.0243\n",
      "13: accuracy:0.74 loss: 84.6853\n",
      "14: accuracy:0.83 loss: 73.4402\n",
      "15: accuracy:0.78 loss: 73.8648\n",
      "16: accuracy:0.84 loss: 72.4589\n",
      "17: accuracy:0.78 loss: 71.3525\n",
      "18: accuracy:0.82 loss: 67.7167\n",
      "19: accuracy:0.78 loss: 70.3038\n",
      "20: accuracy:0.84 loss: 60.6919\n",
      "21: accuracy:0.79 loss: 61.8259\n",
      "22: accuracy:0.81 loss: 62.2855\n",
      "23: accuracy:0.86 loss: 56.7555\n",
      "24: accuracy:0.82 loss: 66.6529\n",
      "25: accuracy:0.85 loss: 66.7546\n",
      "26: accuracy:0.85 loss: 47.9472\n",
      "27: accuracy:0.86 loss: 54.4692\n",
      "28: accuracy:0.86 loss: 49.7939\n",
      "29: accuracy:0.91 loss: 44.6135\n",
      "30: accuracy:0.86 loss: 50.7238\n",
      "31: accuracy:0.81 loss: 64.9109\n",
      "32: accuracy:0.79 loss: 69.8627\n",
      "33: accuracy:0.86 loss: 49.7424\n",
      "34: accuracy:0.85 loss: 57.8535\n",
      "35: accuracy:0.81 loss: 64.6884\n",
      "36: accuracy:0.86 loss: 57.5939\n",
      "37: accuracy:0.86 loss: 43.9114\n",
      "38: accuracy:0.88 loss: 49.9759\n",
      "39: accuracy:0.92 loss: 37.7984\n",
      "40: accuracy:0.89 loss: 42.4233\n",
      "41: accuracy:0.87 loss: 54.0174\n",
      "42: accuracy:0.92 loss: 39.5234\n",
      "43: accuracy:0.9 loss: 47.7609\n",
      "44: accuracy:0.86 loss: 52.2595\n",
      "45: accuracy:0.92 loss: 41.4355\n",
      "46: accuracy:0.81 loss: 59.715\n",
      "47: accuracy:0.88 loss: 41.341\n",
      "48: accuracy:0.86 loss: 50.3962\n",
      "49: accuracy:0.86 loss: 42.3169\n",
      "50: accuracy:0.89 loss: 50.9424\n",
      "51: accuracy:0.89 loss: 41.2554\n",
      "52: accuracy:0.81 loss: 54.0511\n",
      "53: accuracy:0.88 loss: 46.6731\n",
      "54: accuracy:0.93 loss: 32.3063\n",
      "55: accuracy:0.91 loss: 43.6973\n",
      "56: accuracy:0.89 loss: 48.431\n",
      "57: accuracy:0.92 loss: 27.6778\n",
      "58: accuracy:0.93 loss: 33.7578\n",
      "59: accuracy:0.86 loss: 53.243\n",
      "60: accuracy:0.85 loss: 53.0668\n",
      "61: accuracy:0.94 loss: 33.0963\n",
      "62: accuracy:0.91 loss: 32.2347\n",
      "63: accuracy:0.85 loss: 49.3145\n",
      "64: accuracy:0.86 loss: 58.5414\n",
      "65: accuracy:0.91 loss: 37.7556\n",
      "66: accuracy:0.9 loss: 38.0958\n",
      "67: accuracy:0.84 loss: 55.3531\n",
      "68: accuracy:0.92 loss: 35.6786\n",
      "69: accuracy:0.86 loss: 47.8772\n",
      "70: accuracy:0.92 loss: 42.827\n",
      "71: accuracy:0.84 loss: 55.0184\n",
      "72: accuracy:0.86 loss: 48.691\n",
      "73: accuracy:0.87 loss: 46.9528\n",
      "74: accuracy:0.89 loss: 38.1155\n",
      "75: accuracy:0.9 loss: 37.8573\n",
      "76: accuracy:0.93 loss: 29.8621\n",
      "77: accuracy:0.88 loss: 46.9214\n",
      "78: accuracy:0.83 loss: 47.6662\n",
      "79: accuracy:0.87 loss: 52.9868\n",
      "80: accuracy:0.92 loss: 30.3503\n",
      "81: accuracy:0.9 loss: 39.1459\n",
      "82: accuracy:0.85 loss: 46.742\n",
      "83: accuracy:0.91 loss: 31.9344\n",
      "84: accuracy:0.89 loss: 38.794\n",
      "85: accuracy:0.87 loss: 40.1975\n",
      "86: accuracy:0.87 loss: 47.7798\n",
      "87: accuracy:0.86 loss: 45.5404\n",
      "88: accuracy:0.86 loss: 46.964\n",
      "89: accuracy:0.88 loss: 46.9909\n",
      "90: accuracy:0.85 loss: 47.8054\n",
      "91: accuracy:0.87 loss: 43.9692\n",
      "92: accuracy:0.86 loss: 48.0328\n",
      "93: accuracy:0.87 loss: 49.1397\n",
      "94: accuracy:0.87 loss: 46.0428\n",
      "95: accuracy:0.91 loss: 37.3673\n",
      "96: accuracy:0.88 loss: 39.2819\n",
      "97: accuracy:0.87 loss: 51.2761\n",
      "98: accuracy:0.89 loss: 33.7267\n",
      "99: accuracy:0.85 loss: 50.632\n",
      "100: accuracy:0.88 loss: 38.3139\n",
      "101: accuracy:0.91 loss: 38.5127\n",
      "102: accuracy:0.83 loss: 56.7202\n",
      "103: accuracy:0.88 loss: 41.1365\n",
      "104: accuracy:0.88 loss: 37.8854\n",
      "105: accuracy:0.89 loss: 38.6135\n",
      "106: accuracy:0.87 loss: 45.1557\n",
      "107: accuracy:0.86 loss: 37.489\n",
      "108: accuracy:0.88 loss: 43.4412\n",
      "109: accuracy:0.89 loss: 38.0691\n",
      "110: accuracy:0.82 loss: 57.7974\n",
      "111: accuracy:0.91 loss: 32.0206\n",
      "112: accuracy:0.82 loss: 64.0982\n",
      "113: accuracy:0.93 loss: 38.3684\n",
      "114: accuracy:0.87 loss: 39.663\n",
      "115: accuracy:0.94 loss: 27.7871\n",
      "116: accuracy:0.87 loss: 40.2033\n",
      "117: accuracy:0.82 loss: 58.1035\n",
      "118: accuracy:0.87 loss: 38.3887\n",
      "119: accuracy:0.91 loss: 30.0382\n",
      "120: accuracy:0.87 loss: 37.0279\n",
      "121: accuracy:0.89 loss: 38.8557\n",
      "122: accuracy:0.86 loss: 45.8509\n",
      "123: accuracy:0.91 loss: 39.0866\n",
      "124: accuracy:0.89 loss: 39.4525\n",
      "125: accuracy:0.85 loss: 36.1418\n",
      "126: accuracy:0.91 loss: 36.1458\n",
      "127: accuracy:0.9 loss: 42.5039\n",
      "128: accuracy:0.89 loss: 37.1571\n",
      "129: accuracy:0.92 loss: 34.0381\n",
      "130: accuracy:0.91 loss: 32.8089\n",
      "131: accuracy:0.88 loss: 39.706\n",
      "132: accuracy:0.9 loss: 39.1035\n",
      "133: accuracy:0.87 loss: 51.775\n",
      "134: accuracy:0.91 loss: 41.9936\n",
      "135: accuracy:0.86 loss: 44.9365\n",
      "136: accuracy:0.87 loss: 42.4239\n",
      "137: accuracy:0.87 loss: 45.0372\n",
      "138: accuracy:0.87 loss: 37.146\n",
      "139: accuracy:0.9 loss: 34.7125\n",
      "140: accuracy:0.84 loss: 49.9637\n",
      "141: accuracy:0.92 loss: 26.0621\n",
      "142: accuracy:0.91 loss: 35.9739\n",
      "143: accuracy:0.9 loss: 31.9785\n",
      "144: accuracy:0.93 loss: 39.3486\n",
      "145: accuracy:0.93 loss: 30.0627\n",
      "146: accuracy:0.87 loss: 35.2278\n",
      "147: accuracy:0.89 loss: 44.1996\n",
      "148: accuracy:0.9 loss: 34.5014\n",
      "149: accuracy:0.86 loss: 46.7591\n",
      "150: accuracy:0.9 loss: 52.0466\n",
      "151: accuracy:0.91 loss: 30.2334\n",
      "152: accuracy:0.89 loss: 44.4319\n",
      "153: accuracy:0.94 loss: 25.8793\n",
      "154: accuracy:0.85 loss: 48.6825\n",
      "155: accuracy:0.9 loss: 48.7509\n",
      "156: accuracy:0.95 loss: 22.1707\n",
      "157: accuracy:0.9 loss: 38.4781\n",
      "158: accuracy:0.82 loss: 46.034\n",
      "159: accuracy:0.88 loss: 47.0142\n",
      "160: accuracy:0.82 loss: 60.3721\n",
      "161: accuracy:0.88 loss: 43.1948\n",
      "162: accuracy:0.84 loss: 44.7293\n",
      "163: accuracy:0.87 loss: 42.9601\n",
      "164: accuracy:0.87 loss: 40.6162\n",
      "165: accuracy:0.9 loss: 36.6188\n",
      "166: accuracy:0.91 loss: 30.1812\n",
      "167: accuracy:0.91 loss: 26.8829\n",
      "168: accuracy:0.9 loss: 41.8873\n",
      "169: accuracy:0.91 loss: 37.5298\n",
      "170: accuracy:0.9 loss: 31.7343\n",
      "171: accuracy:0.9 loss: 37.3647\n",
      "172: accuracy:0.86 loss: 45.5543\n",
      "173: accuracy:0.9 loss: 40.1749\n",
      "174: accuracy:0.94 loss: 33.8649\n",
      "175: accuracy:0.92 loss: 28.0551\n",
      "176: accuracy:0.9 loss: 31.293\n",
      "177: accuracy:0.84 loss: 54.7915\n",
      "178: accuracy:0.89 loss: 33.5401\n",
      "179: accuracy:0.88 loss: 38.025\n",
      "180: accuracy:0.93 loss: 29.6199\n",
      "181: accuracy:0.82 loss: 51.7025\n",
      "182: accuracy:0.87 loss: 36.8753\n",
      "183: accuracy:0.89 loss: 42.9652\n",
      "184: accuracy:0.87 loss: 40.5477\n",
      "185: accuracy:0.93 loss: 23.8446\n",
      "186: accuracy:0.91 loss: 32.7455\n",
      "187: accuracy:0.86 loss: 35.0075\n",
      "188: accuracy:0.87 loss: 35.8848\n",
      "189: accuracy:0.92 loss: 27.3426\n",
      "190: accuracy:0.91 loss: 24.0716\n",
      "191: accuracy:0.92 loss: 35.294\n",
      "192: accuracy:0.9 loss: 32.1358\n",
      "193: accuracy:0.92 loss: 26.6921\n",
      "194: accuracy:0.91 loss: 29.2447\n",
      "195: accuracy:0.92 loss: 36.3584\n",
      "196: accuracy:0.89 loss: 35.7199\n",
      "197: accuracy:0.92 loss: 36.8879\n",
      "198: accuracy:0.94 loss: 26.5652\n",
      "199: accuracy:0.92 loss: 28.8067\n",
      "200: accuracy:0.93 loss: 27.2302\n",
      "201: accuracy:0.83 loss: 44.9232\n",
      "202: accuracy:0.88 loss: 37.1649\n",
      "203: accuracy:0.85 loss: 61.6387\n",
      "204: accuracy:0.85 loss: 43.968\n",
      "205: accuracy:0.95 loss: 20.2632\n",
      "206: accuracy:0.88 loss: 35.1334\n",
      "207: accuracy:0.93 loss: 25.8547\n",
      "208: accuracy:0.87 loss: 45.6516\n",
      "209: accuracy:0.9 loss: 43.4452\n",
      "210: accuracy:0.88 loss: 31.7597\n",
      "211: accuracy:0.92 loss: 28.4748\n",
      "212: accuracy:0.87 loss: 36.379\n",
      "213: accuracy:0.86 loss: 46.4974\n",
      "214: accuracy:0.95 loss: 23.0113\n",
      "215: accuracy:0.91 loss: 37.1546\n",
      "216: accuracy:0.94 loss: 21.4753\n",
      "217: accuracy:0.94 loss: 32.976\n",
      "218: accuracy:0.94 loss: 20.1901\n",
      "219: accuracy:0.87 loss: 47.3484\n",
      "220: accuracy:0.89 loss: 37.5346\n",
      "221: accuracy:0.91 loss: 25.5839\n",
      "222: accuracy:0.9 loss: 29.7784\n",
      "223: accuracy:0.88 loss: 64.1242\n",
      "224: accuracy:0.92 loss: 33.2622\n",
      "225: accuracy:0.9 loss: 27.3712\n",
      "226: accuracy:0.94 loss: 25.5746\n",
      "227: accuracy:0.91 loss: 27.5522\n",
      "228: accuracy:0.88 loss: 39.7549\n",
      "229: accuracy:0.89 loss: 41.6903\n",
      "230: accuracy:0.89 loss: 45.3154\n",
      "231: accuracy:0.9 loss: 41.1872\n",
      "232: accuracy:0.95 loss: 21.8891\n",
      "233: accuracy:0.93 loss: 36.7717\n",
      "234: accuracy:0.88 loss: 47.4853\n",
      "235: accuracy:0.94 loss: 24.0691\n",
      "236: accuracy:0.9 loss: 34.1374\n",
      "237: accuracy:0.88 loss: 35.4945\n",
      "238: accuracy:0.85 loss: 49.5239\n",
      "239: accuracy:0.94 loss: 33.0733\n",
      "240: accuracy:0.9 loss: 30.8903\n",
      "241: accuracy:0.88 loss: 36.6272\n",
      "242: accuracy:0.84 loss: 56.1417\n",
      "243: accuracy:0.91 loss: 30.5467\n",
      "244: accuracy:0.89 loss: 32.0917\n",
      "245: accuracy:0.89 loss: 38.4476\n",
      "246: accuracy:0.92 loss: 27.8225\n",
      "247: accuracy:0.89 loss: 35.7535\n",
      "248: accuracy:0.88 loss: 42.9903\n",
      "249: accuracy:0.88 loss: 37.3647\n",
      "250: accuracy:0.88 loss: 47.4467\n",
      "251: accuracy:0.89 loss: 33.2272\n",
      "252: accuracy:0.83 loss: 48.1491\n",
      "253: accuracy:0.9 loss: 31.9007\n",
      "254: accuracy:0.83 loss: 44.7554\n",
      "255: accuracy:0.85 loss: 59.5558\n",
      "256: accuracy:0.88 loss: 35.1485\n",
      "257: accuracy:0.93 loss: 24.7563\n",
      "258: accuracy:0.92 loss: 25.0872\n",
      "259: accuracy:0.91 loss: 31.4124\n",
      "260: accuracy:0.89 loss: 35.3879\n",
      "261: accuracy:0.92 loss: 28.5971\n",
      "262: accuracy:0.9 loss: 40.2061\n",
      "263: accuracy:0.92 loss: 24.6361\n",
      "264: accuracy:0.9 loss: 32.9211\n",
      "265: accuracy:0.86 loss: 52.2837\n",
      "266: accuracy:0.87 loss: 57.1979\n",
      "267: accuracy:0.96 loss: 19.3792\n",
      "268: accuracy:0.89 loss: 35.6827\n",
      "269: accuracy:0.85 loss: 51.6914\n",
      "270: accuracy:0.93 loss: 28.7751\n",
      "271: accuracy:0.91 loss: 35.2595\n",
      "272: accuracy:0.89 loss: 32.5405\n",
      "273: accuracy:0.9 loss: 33.3314\n",
      "274: accuracy:0.94 loss: 20.45\n",
      "275: accuracy:0.93 loss: 24.5267\n",
      "276: accuracy:0.9 loss: 32.7516\n",
      "277: accuracy:0.9 loss: 43.7171\n",
      "278: accuracy:0.93 loss: 26.0483\n",
      "279: accuracy:0.94 loss: 28.7454\n",
      "280: accuracy:0.92 loss: 29.5118\n",
      "281: accuracy:0.97 loss: 18.5669\n",
      "282: accuracy:0.84 loss: 39.8878\n",
      "283: accuracy:0.83 loss: 42.5386\n",
      "284: accuracy:0.84 loss: 49.625\n",
      "285: accuracy:0.86 loss: 49.6732\n",
      "286: accuracy:0.95 loss: 26.0461\n",
      "287: accuracy:0.84 loss: 43.7319\n",
      "288: accuracy:0.82 loss: 49.0264\n",
      "289: accuracy:0.89 loss: 32.1155\n",
      "290: accuracy:0.86 loss: 39.9914\n",
      "291: accuracy:0.91 loss: 34.1746\n",
      "292: accuracy:0.86 loss: 50.3713\n",
      "293: accuracy:0.95 loss: 23.448\n",
      "294: accuracy:0.9 loss: 36.6606\n",
      "295: accuracy:0.94 loss: 21.3207\n",
      "296: accuracy:0.87 loss: 36.1581\n",
      "297: accuracy:0.89 loss: 29.4696\n",
      "298: accuracy:0.88 loss: 41.5114\n",
      "299: accuracy:0.9 loss: 26.6337\n",
      "300: accuracy:0.94 loss: 25.3599\n",
      "301: accuracy:0.91 loss: 25.3944\n",
      "302: accuracy:0.88 loss: 45.5562\n",
      "303: accuracy:0.84 loss: 47.6201\n",
      "304: accuracy:0.89 loss: 39.1053\n",
      "305: accuracy:0.91 loss: 23.7387\n",
      "306: accuracy:0.87 loss: 41.4303\n",
      "307: accuracy:0.86 loss: 42.9417\n",
      "308: accuracy:0.92 loss: 34.4951\n",
      "309: accuracy:0.94 loss: 24.7369\n",
      "310: accuracy:0.88 loss: 49.7484\n",
      "311: accuracy:0.88 loss: 35.6171\n",
      "312: accuracy:0.92 loss: 30.0158\n",
      "313: accuracy:0.85 loss: 54.9937\n",
      "314: accuracy:0.91 loss: 26.5949\n",
      "315: accuracy:0.87 loss: 45.6899\n",
      "316: accuracy:0.91 loss: 31.4185\n",
      "317: accuracy:0.9 loss: 37.7565\n",
      "318: accuracy:0.92 loss: 28.9062\n",
      "319: accuracy:0.91 loss: 45.3037\n",
      "320: accuracy:0.93 loss: 31.8496\n",
      "321: accuracy:0.82 loss: 51.6193\n",
      "322: accuracy:0.85 loss: 46.4952\n",
      "323: accuracy:0.89 loss: 36.524\n",
      "324: accuracy:0.88 loss: 46.2337\n",
      "325: accuracy:0.89 loss: 39.1727\n",
      "326: accuracy:0.9 loss: 36.8305\n",
      "327: accuracy:0.92 loss: 30.8449\n",
      "328: accuracy:0.92 loss: 24.6875\n",
      "329: accuracy:0.85 loss: 40.5043\n",
      "330: accuracy:0.88 loss: 44.4355\n",
      "331: accuracy:0.82 loss: 55.1897\n",
      "332: accuracy:0.9 loss: 30.9837\n",
      "333: accuracy:0.88 loss: 46.4467\n",
      "334: accuracy:0.92 loss: 32.4333\n",
      "335: accuracy:0.93 loss: 33.3087\n",
      "336: accuracy:0.87 loss: 35.6526\n",
      "337: accuracy:0.9 loss: 34.345\n",
      "338: accuracy:0.93 loss: 36.9548\n",
      "339: accuracy:0.94 loss: 33.5872\n",
      "340: accuracy:0.93 loss: 21.1263\n",
      "341: accuracy:0.88 loss: 28.3872\n",
      "342: accuracy:0.89 loss: 35.9365\n",
      "343: accuracy:0.87 loss: 46.8008\n",
      "344: accuracy:0.89 loss: 37.1299\n",
      "345: accuracy:0.88 loss: 44.7558\n",
      "346: accuracy:0.87 loss: 44.8121\n",
      "347: accuracy:0.92 loss: 19.1241\n",
      "348: accuracy:0.92 loss: 35.1525\n",
      "349: accuracy:0.96 loss: 32.8151\n",
      "350: accuracy:0.91 loss: 34.0551\n",
      "351: accuracy:0.88 loss: 37.5131\n",
      "352: accuracy:0.89 loss: 44.9713\n",
      "353: accuracy:0.91 loss: 27.7974\n",
      "354: accuracy:0.89 loss: 33.8626\n",
      "355: accuracy:0.87 loss: 39.0809\n",
      "356: accuracy:0.88 loss: 40.058\n",
      "357: accuracy:0.91 loss: 32.8141\n",
      "358: accuracy:0.91 loss: 35.2162\n",
      "359: accuracy:0.9 loss: 33.7867\n",
      "360: accuracy:0.92 loss: 31.2805\n",
      "361: accuracy:0.93 loss: 25.082\n",
      "362: accuracy:0.92 loss: 34.6718\n",
      "363: accuracy:0.88 loss: 37.8841\n",
      "364: accuracy:0.93 loss: 29.2652\n",
      "365: accuracy:0.94 loss: 25.0376\n",
      "366: accuracy:0.92 loss: 31.4982\n",
      "367: accuracy:0.88 loss: 38.7238\n",
      "368: accuracy:0.95 loss: 26.3415\n",
      "369: accuracy:0.86 loss: 45.752\n",
      "370: accuracy:0.83 loss: 40.8155\n",
      "371: accuracy:0.93 loss: 20.2851\n",
      "372: accuracy:0.91 loss: 26.4036\n",
      "373: accuracy:0.94 loss: 26.4965\n",
      "374: accuracy:0.91 loss: 29.2359\n",
      "375: accuracy:0.89 loss: 37.2651\n",
      "376: accuracy:0.87 loss: 30.8341\n",
      "377: accuracy:0.91 loss: 39.3467\n",
      "378: accuracy:0.86 loss: 42.7559\n",
      "379: accuracy:0.94 loss: 20.8703\n",
      "380: accuracy:0.9 loss: 38.5498\n",
      "381: accuracy:0.93 loss: 31.5835\n",
      "382: accuracy:0.87 loss: 32.3517\n",
      "383: accuracy:0.91 loss: 35.7499\n",
      "384: accuracy:0.89 loss: 40.4312\n",
      "385: accuracy:0.87 loss: 38.0168\n",
      "386: accuracy:0.91 loss: 31.4801\n",
      "387: accuracy:0.91 loss: 35.7815\n",
      "388: accuracy:0.86 loss: 54.3549\n",
      "389: accuracy:0.92 loss: 34.3945\n",
      "390: accuracy:0.87 loss: 39.9379\n",
      "391: accuracy:0.84 loss: 54.671\n",
      "392: accuracy:0.89 loss: 38.459\n",
      "393: accuracy:0.92 loss: 30.2069\n",
      "394: accuracy:0.94 loss: 29.4876\n",
      "395: accuracy:0.93 loss: 34.8051\n",
      "396: accuracy:0.9 loss: 32.7687\n",
      "397: accuracy:0.95 loss: 23.2609\n",
      "398: accuracy:0.92 loss: 25.9678\n",
      "399: accuracy:0.89 loss: 34.7729\n",
      "400: accuracy:0.89 loss: 48.7833\n",
      "401: accuracy:0.95 loss: 23.4909\n",
      "402: accuracy:0.89 loss: 36.9823\n",
      "403: accuracy:0.91 loss: 35.0727\n",
      "404: accuracy:0.9 loss: 27.4705\n",
      "405: accuracy:0.94 loss: 27.7776\n",
      "406: accuracy:0.92 loss: 23.8151\n",
      "407: accuracy:0.94 loss: 22.7771\n",
      "408: accuracy:0.88 loss: 36.9778\n",
      "409: accuracy:0.9 loss: 35.7407\n",
      "410: accuracy:0.94 loss: 27.8783\n",
      "411: accuracy:0.9 loss: 37.2369\n",
      "412: accuracy:0.94 loss: 20.8338\n",
      "413: accuracy:0.93 loss: 33.5751\n",
      "414: accuracy:0.94 loss: 24.0644\n",
      "415: accuracy:0.93 loss: 28.2991\n",
      "416: accuracy:0.93 loss: 23.2993\n",
      "417: accuracy:0.94 loss: 20.7964\n",
      "418: accuracy:0.93 loss: 22.6128\n",
      "419: accuracy:0.94 loss: 24.4018\n",
      "420: accuracy:0.93 loss: 32.102\n",
      "421: accuracy:0.88 loss: 40.5021\n",
      "422: accuracy:0.89 loss: 37.9985\n",
      "423: accuracy:0.89 loss: 33.9381\n",
      "424: accuracy:0.94 loss: 23.0578\n",
      "425: accuracy:0.92 loss: 32.6907\n",
      "426: accuracy:0.94 loss: 23.4461\n",
      "427: accuracy:0.86 loss: 40.4456\n",
      "428: accuracy:0.94 loss: 32.3085\n",
      "429: accuracy:0.86 loss: 45.3931\n",
      "430: accuracy:0.89 loss: 34.2113\n",
      "431: accuracy:0.89 loss: 33.5843\n",
      "432: accuracy:0.94 loss: 27.334\n",
      "433: accuracy:0.93 loss: 29.2684\n",
      "434: accuracy:0.94 loss: 21.0534\n",
      "435: accuracy:0.89 loss: 36.8319\n",
      "436: accuracy:0.88 loss: 43.7585\n",
      "437: accuracy:0.92 loss: 28.3923\n",
      "438: accuracy:0.91 loss: 25.7136\n",
      "439: accuracy:0.87 loss: 50.3326\n",
      "440: accuracy:0.95 loss: 23.1864\n",
      "441: accuracy:0.92 loss: 30.8124\n",
      "442: accuracy:0.86 loss: 42.5507\n",
      "443: accuracy:0.85 loss: 53.331\n",
      "444: accuracy:0.93 loss: 24.6533\n",
      "445: accuracy:0.91 loss: 42.8294\n",
      "446: accuracy:0.91 loss: 28.0407\n",
      "447: accuracy:0.85 loss: 37.7864\n",
      "448: accuracy:0.91 loss: 38.6189\n",
      "449: accuracy:0.85 loss: 44.8827\n",
      "450: accuracy:0.94 loss: 24.1896\n",
      "451: accuracy:0.89 loss: 31.2789\n",
      "452: accuracy:0.94 loss: 26.9734\n",
      "453: accuracy:0.88 loss: 35.5162\n",
      "454: accuracy:0.8 loss: 50.7976\n",
      "455: accuracy:0.9 loss: 33.5293\n",
      "456: accuracy:0.92 loss: 32.7826\n",
      "457: accuracy:0.89 loss: 26.9697\n",
      "458: accuracy:0.9 loss: 28.051\n",
      "459: accuracy:0.92 loss: 30.9951\n",
      "460: accuracy:0.92 loss: 25.2005\n",
      "461: accuracy:0.91 loss: 35.1512\n",
      "462: accuracy:0.91 loss: 33.7865\n",
      "463: accuracy:0.89 loss: 37.6344\n",
      "464: accuracy:0.86 loss: 43.9501\n",
      "465: accuracy:0.86 loss: 48.2671\n",
      "466: accuracy:0.95 loss: 18.0449\n",
      "467: accuracy:0.9 loss: 34.0221\n",
      "468: accuracy:0.89 loss: 31.454\n",
      "469: accuracy:0.92 loss: 28.2379\n",
      "470: accuracy:0.9 loss: 37.8312\n",
      "471: accuracy:0.94 loss: 24.7995\n",
      "472: accuracy:0.93 loss: 27.8376\n",
      "473: accuracy:0.86 loss: 45.2386\n",
      "474: accuracy:0.9 loss: 29.6021\n",
      "475: accuracy:0.92 loss: 34.8287\n",
      "476: accuracy:0.9 loss: 32.6739\n",
      "477: accuracy:0.94 loss: 29.1671\n",
      "478: accuracy:0.89 loss: 34.5012\n",
      "479: accuracy:0.88 loss: 34.9569\n",
      "480: accuracy:0.93 loss: 20.5517\n",
      "481: accuracy:0.95 loss: 28.0458\n",
      "482: accuracy:0.86 loss: 47.5927\n",
      "483: accuracy:0.92 loss: 27.0619\n",
      "484: accuracy:0.9 loss: 30.0415\n",
      "485: accuracy:0.88 loss: 36.1959\n",
      "486: accuracy:0.86 loss: 37.0849\n",
      "487: accuracy:0.94 loss: 26.6311\n",
      "488: accuracy:0.89 loss: 39.5631\n",
      "489: accuracy:0.93 loss: 25.0432\n",
      "490: accuracy:0.93 loss: 17.6542\n",
      "491: accuracy:0.87 loss: 44.5436\n",
      "492: accuracy:0.9 loss: 32.061\n",
      "493: accuracy:0.88 loss: 44.5911\n",
      "494: accuracy:0.9 loss: 36.4509\n",
      "495: accuracy:0.93 loss: 26.465\n",
      "496: accuracy:0.91 loss: 39.9549\n",
      "497: accuracy:0.92 loss: 26.9926\n",
      "498: accuracy:0.92 loss: 28.5881\n",
      "499: accuracy:0.91 loss: 38.4948\n",
      "500: accuracy:0.83 loss: 42.2132\n",
      "501: accuracy:0.89 loss: 30.2273\n",
      "502: accuracy:0.92 loss: 33.528\n",
      "503: accuracy:0.9 loss: 35.8885\n",
      "504: accuracy:0.9 loss: 31.7975\n",
      "505: accuracy:0.92 loss: 23.2334\n",
      "506: accuracy:0.91 loss: 26.0717\n",
      "507: accuracy:0.91 loss: 32.5774\n",
      "508: accuracy:0.92 loss: 34.11\n",
      "509: accuracy:0.91 loss: 34.5655\n",
      "510: accuracy:0.88 loss: 28.5138\n",
      "511: accuracy:0.87 loss: 47.9153\n",
      "512: accuracy:0.89 loss: 33.9609\n",
      "513: accuracy:0.86 loss: 46.7581\n",
      "514: accuracy:0.92 loss: 23.181\n",
      "515: accuracy:0.92 loss: 40.2477\n",
      "516: accuracy:0.89 loss: 31.4847\n",
      "517: accuracy:0.95 loss: 16.1003\n",
      "518: accuracy:0.86 loss: 44.1071\n",
      "519: accuracy:0.93 loss: 21.6596\n",
      "520: accuracy:0.88 loss: 38.6349\n",
      "521: accuracy:0.92 loss: 26.1238\n",
      "522: accuracy:0.92 loss: 28.6275\n",
      "523: accuracy:0.89 loss: 35.6806\n",
      "524: accuracy:0.92 loss: 24.2234\n",
      "525: accuracy:0.94 loss: 19.8062\n",
      "526: accuracy:0.92 loss: 26.9424\n",
      "527: accuracy:0.91 loss: 29.3231\n",
      "528: accuracy:0.93 loss: 32.1046\n",
      "529: accuracy:0.91 loss: 41.9807\n",
      "530: accuracy:0.93 loss: 24.7049\n",
      "531: accuracy:0.83 loss: 41.3506\n",
      "532: accuracy:0.9 loss: 30.7765\n",
      "533: accuracy:0.94 loss: 27.593\n",
      "534: accuracy:0.89 loss: 44.8392\n",
      "535: accuracy:0.91 loss: 24.1045\n",
      "536: accuracy:0.83 loss: 48.8\n",
      "537: accuracy:0.87 loss: 60.5856\n",
      "538: accuracy:0.92 loss: 26.3675\n",
      "539: accuracy:0.94 loss: 17.9489\n",
      "540: accuracy:0.89 loss: 27.3335\n",
      "541: accuracy:0.95 loss: 18.1716\n",
      "542: accuracy:0.9 loss: 27.6347\n",
      "543: accuracy:0.89 loss: 31.1166\n",
      "544: accuracy:0.92 loss: 37.153\n",
      "545: accuracy:0.89 loss: 37.7338\n",
      "546: accuracy:0.92 loss: 30.3386\n",
      "547: accuracy:0.92 loss: 33.5996\n",
      "548: accuracy:0.92 loss: 26.8378\n",
      "549: accuracy:0.94 loss: 22.6223\n",
      "550: accuracy:0.97 loss: 27.9191\n",
      "551: accuracy:0.87 loss: 28.4246\n",
      "552: accuracy:0.88 loss: 36.1532\n",
      "553: accuracy:0.94 loss: 21.5372\n",
      "554: accuracy:0.95 loss: 28.5601\n",
      "555: accuracy:0.9 loss: 32.7861\n",
      "556: accuracy:0.92 loss: 26.6326\n",
      "557: accuracy:0.91 loss: 20.2716\n",
      "558: accuracy:0.89 loss: 38.7465\n",
      "559: accuracy:0.88 loss: 41.5902\n",
      "560: accuracy:0.9 loss: 33.5336\n",
      "561: accuracy:0.83 loss: 48.3453\n",
      "562: accuracy:0.93 loss: 32.5672\n",
      "563: accuracy:0.91 loss: 33.023\n",
      "564: accuracy:0.9 loss: 34.5107\n",
      "565: accuracy:0.91 loss: 29.3805\n",
      "566: accuracy:0.84 loss: 45.3252\n",
      "567: accuracy:0.87 loss: 62.0426\n",
      "568: accuracy:0.91 loss: 26.9474\n",
      "569: accuracy:0.89 loss: 35.5037\n",
      "570: accuracy:0.9 loss: 31.4097\n",
      "571: accuracy:0.91 loss: 32.455\n",
      "572: accuracy:0.89 loss: 31.1514\n",
      "573: accuracy:0.91 loss: 30.4204\n",
      "574: accuracy:0.9 loss: 35.0596\n",
      "575: accuracy:0.95 loss: 21.6704\n",
      "576: accuracy:0.92 loss: 26.6968\n",
      "577: accuracy:0.91 loss: 47.807\n",
      "578: accuracy:0.91 loss: 23.5444\n",
      "579: accuracy:0.85 loss: 65.3539\n",
      "580: accuracy:0.89 loss: 36.0912\n",
      "581: accuracy:0.92 loss: 29.1849\n",
      "582: accuracy:0.87 loss: 36.6284\n",
      "583: accuracy:0.91 loss: 43.8502\n",
      "584: accuracy:0.94 loss: 22.5667\n",
      "585: accuracy:0.9 loss: 33.0046\n",
      "586: accuracy:0.89 loss: 33.0391\n",
      "587: accuracy:0.86 loss: 49.275\n",
      "588: accuracy:0.91 loss: 22.1428\n",
      "589: accuracy:0.93 loss: 23.4803\n",
      "590: accuracy:0.93 loss: 23.6962\n",
      "591: accuracy:0.9 loss: 28.1951\n",
      "592: accuracy:0.93 loss: 27.6061\n",
      "593: accuracy:0.91 loss: 30.8329\n",
      "594: accuracy:0.9 loss: 38.6726\n",
      "595: accuracy:0.9 loss: 29.3319\n",
      "596: accuracy:0.89 loss: 33.0458\n",
      "597: accuracy:0.9 loss: 31.1077\n",
      "598: accuracy:0.83 loss: 55.1113\n",
      "599: accuracy:0.92 loss: 33.4333\n",
      "600: accuracy:0.89 loss: 39.5351\n",
      "601: accuracy:0.91 loss: 33.5233\n",
      "602: accuracy:0.93 loss: 22.6618\n",
      "603: accuracy:0.93 loss: 23.0183\n",
      "604: accuracy:0.9 loss: 28.6657\n",
      "605: accuracy:0.89 loss: 38.8382\n",
      "606: accuracy:0.92 loss: 24.5352\n",
      "607: accuracy:0.91 loss: 24.1364\n",
      "608: accuracy:0.88 loss: 45.0207\n",
      "609: accuracy:0.9 loss: 45.7134\n",
      "610: accuracy:0.88 loss: 43.3351\n",
      "611: accuracy:0.92 loss: 27.1188\n",
      "612: accuracy:0.91 loss: 34.7901\n",
      "613: accuracy:0.93 loss: 23.6403\n",
      "614: accuracy:0.93 loss: 24.1956\n",
      "615: accuracy:0.89 loss: 27.3403\n",
      "616: accuracy:0.96 loss: 20.5154\n",
      "617: accuracy:0.9 loss: 48.9258\n",
      "618: accuracy:0.89 loss: 46.0024\n",
      "619: accuracy:0.89 loss: 35.076\n",
      "620: accuracy:0.94 loss: 22.9568\n",
      "621: accuracy:0.91 loss: 35.8559\n",
      "622: accuracy:0.93 loss: 27.6454\n",
      "623: accuracy:0.9 loss: 32.6552\n",
      "624: accuracy:0.89 loss: 28.5539\n",
      "625: accuracy:0.93 loss: 21.1391\n",
      "626: accuracy:0.96 loss: 23.8989\n",
      "627: accuracy:0.93 loss: 21.6989\n",
      "628: accuracy:0.94 loss: 26.473\n",
      "629: accuracy:0.95 loss: 22.0374\n",
      "630: accuracy:0.87 loss: 30.549\n",
      "631: accuracy:0.94 loss: 24.5281\n",
      "632: accuracy:0.95 loss: 24.5706\n",
      "633: accuracy:0.89 loss: 29.9667\n",
      "634: accuracy:0.93 loss: 30.8233\n",
      "635: accuracy:0.94 loss: 17.5216\n",
      "636: accuracy:0.94 loss: 19.9338\n",
      "637: accuracy:0.9 loss: 30.784\n",
      "638: accuracy:0.92 loss: 27.3789\n",
      "639: accuracy:0.93 loss: 36.6632\n",
      "640: accuracy:0.91 loss: 25.2481\n",
      "641: accuracy:0.93 loss: 20.9841\n",
      "642: accuracy:0.86 loss: 45.4065\n",
      "643: accuracy:0.86 loss: 29.5634\n",
      "644: accuracy:0.93 loss: 21.8459\n",
      "645: accuracy:0.88 loss: 35.0306\n",
      "646: accuracy:0.9 loss: 31.6013\n",
      "647: accuracy:0.92 loss: 24.4068\n",
      "648: accuracy:0.9 loss: 30.2935\n",
      "649: accuracy:0.91 loss: 39.9142\n",
      "650: accuracy:0.95 loss: 28.1436\n",
      "651: accuracy:0.91 loss: 34.0572\n",
      "652: accuracy:0.92 loss: 26.6431\n",
      "653: accuracy:0.91 loss: 28.14\n",
      "654: accuracy:0.92 loss: 36.5106\n",
      "655: accuracy:0.93 loss: 24.4122\n",
      "656: accuracy:0.9 loss: 47.7604\n",
      "657: accuracy:0.95 loss: 28.0048\n",
      "658: accuracy:0.9 loss: 44.4475\n",
      "659: accuracy:0.94 loss: 20.0444\n",
      "660: accuracy:0.85 loss: 41.1167\n",
      "661: accuracy:0.88 loss: 45.0558\n",
      "662: accuracy:0.95 loss: 22.5751\n",
      "663: accuracy:0.91 loss: 22.1329\n",
      "664: accuracy:0.87 loss: 41.0504\n",
      "665: accuracy:0.93 loss: 25.7931\n",
      "666: accuracy:0.95 loss: 27.9744\n",
      "667: accuracy:0.97 loss: 22.7042\n",
      "668: accuracy:0.87 loss: 47.1112\n",
      "669: accuracy:0.9 loss: 36.0451\n",
      "670: accuracy:0.87 loss: 35.3535\n",
      "671: accuracy:0.9 loss: 38.1787\n",
      "672: accuracy:0.92 loss: 31.0064\n",
      "673: accuracy:0.87 loss: 34.7637\n",
      "674: accuracy:0.91 loss: 21.8042\n",
      "675: accuracy:0.96 loss: 16.5037\n",
      "676: accuracy:0.95 loss: 20.5143\n",
      "677: accuracy:0.9 loss: 35.2122\n",
      "678: accuracy:0.91 loss: 24.1116\n",
      "679: accuracy:0.93 loss: 22.8533\n",
      "680: accuracy:0.92 loss: 25.9512\n",
      "681: accuracy:0.87 loss: 44.2929\n",
      "682: accuracy:0.93 loss: 37.7617\n",
      "683: accuracy:0.92 loss: 33.1478\n",
      "684: accuracy:0.93 loss: 24.192\n",
      "685: accuracy:0.9 loss: 36.543\n",
      "686: accuracy:0.92 loss: 21.2206\n",
      "687: accuracy:0.94 loss: 23.6211\n",
      "688: accuracy:0.94 loss: 21.6227\n",
      "689: accuracy:0.88 loss: 41.1527\n",
      "690: accuracy:0.92 loss: 38.0255\n",
      "691: accuracy:0.86 loss: 42.9383\n",
      "692: accuracy:0.89 loss: 28.121\n",
      "693: accuracy:0.95 loss: 23.8205\n",
      "694: accuracy:0.93 loss: 22.4972\n",
      "695: accuracy:0.88 loss: 40.9677\n",
      "696: accuracy:0.91 loss: 28.5617\n",
      "697: accuracy:0.89 loss: 32.848\n",
      "698: accuracy:0.91 loss: 27.662\n",
      "699: accuracy:0.9 loss: 34.1796\n",
      "700: accuracy:0.95 loss: 22.8736\n",
      "701: accuracy:0.9 loss: 34.7012\n",
      "702: accuracy:0.95 loss: 23.1056\n",
      "703: accuracy:0.87 loss: 40.943\n",
      "704: accuracy:0.91 loss: 37.9295\n",
      "705: accuracy:0.88 loss: 35.7462\n",
      "706: accuracy:0.89 loss: 45.8884\n",
      "707: accuracy:0.87 loss: 45.0547\n",
      "708: accuracy:0.94 loss: 18.7252\n",
      "709: accuracy:0.9 loss: 50.4623\n",
      "710: accuracy:0.96 loss: 15.6282\n",
      "711: accuracy:0.92 loss: 26.3916\n",
      "712: accuracy:0.87 loss: 34.4945\n",
      "713: accuracy:0.91 loss: 28.2526\n",
      "714: accuracy:0.9 loss: 36.3555\n",
      "715: accuracy:0.91 loss: 25.4161\n",
      "716: accuracy:0.89 loss: 43.8393\n",
      "717: accuracy:0.9 loss: 30.5897\n",
      "718: accuracy:0.91 loss: 24.3236\n",
      "719: accuracy:0.88 loss: 37.1657\n",
      "720: accuracy:0.93 loss: 26.2027\n",
      "721: accuracy:0.89 loss: 32.4347\n",
      "722: accuracy:0.92 loss: 42.2965\n",
      "723: accuracy:0.92 loss: 29.5936\n",
      "724: accuracy:0.87 loss: 36.6175\n",
      "725: accuracy:0.96 loss: 30.6658\n",
      "726: accuracy:0.9 loss: 34.1475\n",
      "727: accuracy:0.91 loss: 31.3392\n",
      "728: accuracy:0.94 loss: 22.2242\n",
      "729: accuracy:0.88 loss: 36.957\n",
      "730: accuracy:0.94 loss: 28.9858\n",
      "731: accuracy:0.94 loss: 25.297\n",
      "732: accuracy:0.9 loss: 31.7191\n",
      "733: accuracy:0.92 loss: 22.8605\n",
      "734: accuracy:0.95 loss: 18.8339\n",
      "735: accuracy:0.88 loss: 33.0632\n",
      "736: accuracy:0.94 loss: 22.5936\n",
      "737: accuracy:0.94 loss: 24.7615\n",
      "738: accuracy:0.92 loss: 23.754\n",
      "739: accuracy:0.91 loss: 29.3114\n",
      "740: accuracy:0.91 loss: 36.7202\n",
      "741: accuracy:0.9 loss: 38.289\n",
      "742: accuracy:0.9 loss: 32.6896\n",
      "743: accuracy:0.97 loss: 14.3201\n",
      "744: accuracy:0.95 loss: 17.5137\n",
      "745: accuracy:0.94 loss: 28.1607\n",
      "746: accuracy:0.89 loss: 42.7302\n",
      "747: accuracy:0.9 loss: 49.2023\n",
      "748: accuracy:0.91 loss: 30.2004\n",
      "749: accuracy:0.89 loss: 31.7097\n",
      "750: accuracy:0.93 loss: 26.2408\n",
      "751: accuracy:0.91 loss: 27.1096\n",
      "752: accuracy:0.93 loss: 21.0547\n",
      "753: accuracy:0.91 loss: 26.468\n",
      "754: accuracy:0.92 loss: 23.3681\n",
      "755: accuracy:0.85 loss: 40.3423\n",
      "756: accuracy:0.95 loss: 18.9726\n",
      "757: accuracy:0.96 loss: 18.3934\n",
      "758: accuracy:0.93 loss: 23.7219\n",
      "759: accuracy:0.93 loss: 22.6883\n",
      "760: accuracy:0.93 loss: 25.4339\n",
      "761: accuracy:0.91 loss: 27.5942\n",
      "762: accuracy:0.95 loss: 22.5268\n",
      "763: accuracy:0.94 loss: 19.6423\n",
      "764: accuracy:0.94 loss: 31.1982\n",
      "765: accuracy:0.92 loss: 34.7827\n",
      "766: accuracy:0.94 loss: 28.3671\n",
      "767: accuracy:0.91 loss: 27.7883\n",
      "768: accuracy:0.89 loss: 39.5601\n",
      "769: accuracy:0.89 loss: 42.5212\n",
      "770: accuracy:0.97 loss: 21.7088\n",
      "771: accuracy:0.89 loss: 31.3126\n",
      "772: accuracy:0.9 loss: 28.9913\n",
      "773: accuracy:0.89 loss: 34.1174\n",
      "774: accuracy:0.89 loss: 30.5516\n",
      "775: accuracy:0.91 loss: 26.4304\n",
      "776: accuracy:0.91 loss: 32.0052\n",
      "777: accuracy:0.93 loss: 25.3214\n",
      "778: accuracy:0.9 loss: 35.7906\n",
      "779: accuracy:0.93 loss: 21.4143\n",
      "780: accuracy:0.9 loss: 25.8667\n",
      "781: accuracy:0.9 loss: 31.4596\n",
      "782: accuracy:0.97 loss: 11.8144\n",
      "783: accuracy:0.9 loss: 34.0522\n",
      "784: accuracy:0.94 loss: 18.427\n",
      "785: accuracy:0.93 loss: 20.4145\n",
      "786: accuracy:0.87 loss: 42.1379\n",
      "787: accuracy:0.9 loss: 33.3357\n",
      "788: accuracy:0.91 loss: 35.5806\n",
      "789: accuracy:0.94 loss: 18.2082\n",
      "790: accuracy:0.88 loss: 28.9171\n",
      "791: accuracy:0.87 loss: 37.5903\n",
      "792: accuracy:0.87 loss: 32.8831\n",
      "793: accuracy:0.92 loss: 31.2444\n",
      "794: accuracy:0.94 loss: 30.1018\n",
      "795: accuracy:0.93 loss: 26.359\n",
      "796: accuracy:0.94 loss: 25.4202\n",
      "797: accuracy:0.95 loss: 19.1146\n",
      "798: accuracy:0.92 loss: 33.9101\n",
      "799: accuracy:0.95 loss: 21.4402\n",
      "800: accuracy:0.91 loss: 23.2635\n",
      "801: accuracy:0.9 loss: 43.8412\n",
      "802: accuracy:0.92 loss: 27.1428\n",
      "803: accuracy:0.91 loss: 33.1627\n",
      "804: accuracy:0.96 loss: 21.1723\n",
      "805: accuracy:0.93 loss: 21.9488\n",
      "806: accuracy:0.95 loss: 20.0078\n",
      "807: accuracy:0.92 loss: 30.9967\n",
      "808: accuracy:0.99 loss: 12.8689\n",
      "809: accuracy:0.89 loss: 29.5307\n",
      "810: accuracy:0.91 loss: 29.9725\n",
      "811: accuracy:0.92 loss: 21.9179\n",
      "812: accuracy:0.87 loss: 31.4967\n",
      "813: accuracy:0.9 loss: 29.5344\n",
      "814: accuracy:0.92 loss: 30.3216\n",
      "815: accuracy:0.92 loss: 26.9752\n",
      "816: accuracy:0.9 loss: 32.6866\n",
      "817: accuracy:0.88 loss: 50.2812\n",
      "818: accuracy:0.86 loss: 35.7305\n",
      "819: accuracy:0.93 loss: 24.4823\n",
      "820: accuracy:0.94 loss: 28.5888\n",
      "821: accuracy:0.93 loss: 21.2421\n",
      "822: accuracy:0.95 loss: 17.903\n",
      "823: accuracy:0.95 loss: 20.9574\n",
      "824: accuracy:0.9 loss: 31.8142\n",
      "825: accuracy:0.93 loss: 26.9704\n",
      "826: accuracy:0.87 loss: 44.4354\n",
      "827: accuracy:0.89 loss: 38.123\n",
      "828: accuracy:0.88 loss: 49.8541\n",
      "829: accuracy:0.93 loss: 23.0111\n",
      "830: accuracy:0.92 loss: 22.5966\n",
      "831: accuracy:0.89 loss: 39.5873\n",
      "832: accuracy:0.94 loss: 22.917\n",
      "833: accuracy:0.95 loss: 26.2704\n",
      "834: accuracy:0.94 loss: 21.4693\n",
      "835: accuracy:0.88 loss: 50.6597\n",
      "836: accuracy:0.88 loss: 57.5823\n",
      "837: accuracy:0.93 loss: 20.1275\n",
      "838: accuracy:0.92 loss: 23.9876\n",
      "839: accuracy:0.88 loss: 42.1839\n",
      "840: accuracy:0.92 loss: 25.3041\n",
      "841: accuracy:0.85 loss: 48.1327\n",
      "842: accuracy:0.95 loss: 18.7329\n",
      "843: accuracy:0.95 loss: 25.9955\n",
      "844: accuracy:0.95 loss: 24.8016\n",
      "845: accuracy:0.9 loss: 24.5934\n",
      "846: accuracy:0.97 loss: 14.3658\n",
      "847: accuracy:0.89 loss: 55.6785\n",
      "848: accuracy:0.86 loss: 48.1023\n",
      "849: accuracy:0.96 loss: 24.8213\n",
      "850: accuracy:0.88 loss: 38.2179\n",
      "851: accuracy:0.92 loss: 39.9711\n",
      "852: accuracy:0.94 loss: 27.1554\n",
      "853: accuracy:0.89 loss: 37.8872\n",
      "854: accuracy:0.89 loss: 38.1394\n",
      "855: accuracy:0.91 loss: 26.472\n",
      "856: accuracy:0.94 loss: 23.6497\n",
      "857: accuracy:0.88 loss: 33.7812\n",
      "858: accuracy:0.88 loss: 42.8251\n",
      "859: accuracy:0.9 loss: 32.4698\n",
      "860: accuracy:0.83 loss: 42.5288\n",
      "861: accuracy:0.87 loss: 39.044\n",
      "862: accuracy:0.9 loss: 49.3851\n",
      "863: accuracy:0.91 loss: 23.9488\n",
      "864: accuracy:0.86 loss: 50.7474\n",
      "865: accuracy:0.93 loss: 29.374\n",
      "866: accuracy:0.94 loss: 28.9117\n",
      "867: accuracy:0.94 loss: 21.6013\n",
      "868: accuracy:0.92 loss: 38.8328\n",
      "869: accuracy:0.91 loss: 29.2851\n",
      "870: accuracy:0.93 loss: 24.4061\n",
      "871: accuracy:0.93 loss: 19.2928\n",
      "872: accuracy:0.87 loss: 39.8092\n",
      "873: accuracy:0.96 loss: 22.1801\n",
      "874: accuracy:0.9 loss: 36.6557\n",
      "875: accuracy:0.92 loss: 30.1392\n",
      "876: accuracy:0.86 loss: 40.0851\n",
      "877: accuracy:0.94 loss: 25.4457\n",
      "878: accuracy:0.94 loss: 25.2303\n",
      "879: accuracy:0.91 loss: 36.7838\n",
      "880: accuracy:0.87 loss: 39.1339\n",
      "881: accuracy:0.93 loss: 21.2819\n",
      "882: accuracy:0.91 loss: 34.1264\n",
      "883: accuracy:0.89 loss: 41.4806\n",
      "884: accuracy:0.9 loss: 28.2671\n",
      "885: accuracy:0.91 loss: 40.9227\n",
      "886: accuracy:0.85 loss: 52.7013\n",
      "887: accuracy:0.84 loss: 37.434\n",
      "888: accuracy:0.87 loss: 32.6857\n",
      "889: accuracy:0.93 loss: 27.9773\n",
      "890: accuracy:0.86 loss: 51.1349\n",
      "891: accuracy:0.91 loss: 29.3799\n",
      "892: accuracy:0.92 loss: 20.8291\n",
      "893: accuracy:0.92 loss: 25.8352\n",
      "894: accuracy:0.84 loss: 40.4786\n",
      "895: accuracy:0.93 loss: 31.236\n",
      "896: accuracy:0.91 loss: 26.0052\n",
      "897: accuracy:0.9 loss: 43.5758\n",
      "898: accuracy:0.95 loss: 18.5658\n",
      "899: accuracy:0.93 loss: 24.0978\n",
      "900: accuracy:0.88 loss: 38.9072\n",
      "901: accuracy:0.92 loss: 40.8066\n",
      "902: accuracy:0.9 loss: 32.4073\n",
      "903: accuracy:0.93 loss: 30.7112\n",
      "904: accuracy:0.93 loss: 25.3746\n",
      "905: accuracy:0.93 loss: 23.9649\n",
      "906: accuracy:0.94 loss: 20.602\n",
      "907: accuracy:0.92 loss: 40.9778\n",
      "908: accuracy:0.93 loss: 23.13\n",
      "909: accuracy:0.85 loss: 42.584\n",
      "910: accuracy:0.88 loss: 37.8037\n",
      "911: accuracy:0.93 loss: 22.6849\n",
      "912: accuracy:0.85 loss: 44.6077\n",
      "913: accuracy:0.94 loss: 23.2181\n",
      "914: accuracy:0.92 loss: 29.3906\n",
      "915: accuracy:0.94 loss: 18.1254\n",
      "916: accuracy:0.89 loss: 43.5143\n",
      "917: accuracy:0.95 loss: 22.3996\n",
      "918: accuracy:0.93 loss: 27.8126\n",
      "919: accuracy:0.92 loss: 30.7443\n",
      "920: accuracy:0.9 loss: 29.7893\n",
      "921: accuracy:0.91 loss: 36.1448\n",
      "922: accuracy:0.95 loss: 17.8613\n",
      "923: accuracy:0.89 loss: 37.1096\n",
      "924: accuracy:0.9 loss: 27.5098\n",
      "925: accuracy:0.88 loss: 29.0184\n",
      "926: accuracy:0.85 loss: 28.2641\n",
      "927: accuracy:0.86 loss: 37.1974\n",
      "928: accuracy:0.89 loss: 44.5341\n",
      "929: accuracy:0.88 loss: 46.0769\n",
      "930: accuracy:0.93 loss: 23.0871\n",
      "931: accuracy:0.89 loss: 43.7832\n",
      "932: accuracy:0.84 loss: 43.689\n",
      "933: accuracy:0.93 loss: 18.902\n",
      "934: accuracy:0.92 loss: 29.6559\n",
      "935: accuracy:0.92 loss: 26.6358\n",
      "936: accuracy:0.93 loss: 29.0437\n",
      "937: accuracy:0.91 loss: 26.6234\n",
      "938: accuracy:0.89 loss: 32.1675\n",
      "939: accuracy:0.88 loss: 47.9901\n",
      "940: accuracy:0.93 loss: 26.3118\n",
      "941: accuracy:0.9 loss: 34.9504\n",
      "942: accuracy:0.89 loss: 49.317\n",
      "943: accuracy:0.9 loss: 32.1789\n",
      "944: accuracy:0.97 loss: 22.98\n",
      "945: accuracy:0.91 loss: 42.7253\n",
      "946: accuracy:0.93 loss: 23.2539\n",
      "947: accuracy:0.94 loss: 22.4263\n",
      "948: accuracy:0.89 loss: 26.9735\n",
      "949: accuracy:0.95 loss: 14.4335\n",
      "950: accuracy:0.95 loss: 16.5208\n",
      "951: accuracy:0.9 loss: 33.5463\n",
      "952: accuracy:0.9 loss: 28.2635\n",
      "953: accuracy:0.88 loss: 41.4552\n",
      "954: accuracy:0.92 loss: 27.6864\n",
      "955: accuracy:0.89 loss: 41.2167\n",
      "956: accuracy:0.9 loss: 37.9136\n",
      "957: accuracy:0.93 loss: 27.1323\n",
      "958: accuracy:0.9 loss: 39.4641\n",
      "959: accuracy:0.9 loss: 34.1349\n",
      "960: accuracy:0.94 loss: 30.0598\n",
      "961: accuracy:0.9 loss: 31.8777\n",
      "962: accuracy:0.88 loss: 38.3763\n",
      "963: accuracy:0.93 loss: 26.114\n",
      "964: accuracy:0.93 loss: 24.7671\n",
      "965: accuracy:0.9 loss: 39.5528\n",
      "966: accuracy:0.89 loss: 32.2143\n",
      "967: accuracy:0.88 loss: 38.5649\n",
      "968: accuracy:0.93 loss: 22.7477\n",
      "969: accuracy:0.89 loss: 33.4896\n",
      "970: accuracy:0.9 loss: 22.5417\n",
      "971: accuracy:0.9 loss: 31.7757\n",
      "972: accuracy:0.94 loss: 19.7632\n",
      "973: accuracy:0.89 loss: 42.0754\n",
      "974: accuracy:0.88 loss: 35.4175\n",
      "975: accuracy:0.93 loss: 23.4675\n",
      "976: accuracy:0.93 loss: 24.1074\n",
      "977: accuracy:0.93 loss: 24.935\n",
      "978: accuracy:0.91 loss: 34.6756\n",
      "979: accuracy:0.9 loss: 22.9453\n",
      "980: accuracy:0.9 loss: 25.6318\n",
      "981: accuracy:0.91 loss: 33.6524\n",
      "982: accuracy:0.89 loss: 38.4717\n",
      "983: accuracy:0.88 loss: 35.5284\n",
      "984: accuracy:0.97 loss: 15.2141\n",
      "985: accuracy:0.9 loss: 35.2913\n",
      "986: accuracy:0.92 loss: 33.0122\n",
      "987: accuracy:0.93 loss: 21.9646\n",
      "988: accuracy:0.89 loss: 45.303\n",
      "989: accuracy:0.84 loss: 53.8206\n",
      "990: accuracy:0.94 loss: 26.5604\n",
      "991: accuracy:0.95 loss: 20.7962\n",
      "992: accuracy:0.93 loss: 33.1967\n",
      "993: accuracy:0.91 loss: 29.2155\n",
      "994: accuracy:0.91 loss: 33.5899\n",
      "995: accuracy:0.91 loss: 34.3015\n",
      "996: accuracy:0.94 loss: 38.2003\n",
      "997: accuracy:0.9 loss: 26.2785\n",
      "998: accuracy:0.92 loss: 28.8111\n",
      "999: accuracy:0.89 loss: 49.4111\n",
      "1000: accuracy:0.94 loss: 21.7526\n",
      "1001: accuracy:0.91 loss: 31.3741\n",
      "1002: accuracy:0.91 loss: 33.11\n",
      "1003: accuracy:0.95 loss: 23.4962\n",
      "1004: accuracy:0.91 loss: 33.2273\n",
      "1005: accuracy:0.93 loss: 38.1437\n",
      "1006: accuracy:0.89 loss: 38.2685\n",
      "1007: accuracy:0.88 loss: 38.9421\n",
      "1008: accuracy:0.94 loss: 22.2138\n",
      "1009: accuracy:0.9 loss: 32.7069\n",
      "1010: accuracy:0.89 loss: 32.8662\n",
      "1011: accuracy:0.96 loss: 29.5592\n",
      "1012: accuracy:0.86 loss: 54.0281\n",
      "1013: accuracy:0.91 loss: 31.2331\n",
      "1014: accuracy:0.89 loss: 45.3115\n",
      "1015: accuracy:0.88 loss: 43.1374\n",
      "1016: accuracy:0.91 loss: 26.7882\n",
      "1017: accuracy:0.91 loss: 23.6823\n",
      "1018: accuracy:0.91 loss: 40.1565\n",
      "1019: accuracy:0.93 loss: 34.3192\n",
      "1020: accuracy:0.92 loss: 30.4521\n",
      "1021: accuracy:0.97 loss: 20.6039\n",
      "1022: accuracy:0.9 loss: 35.8697\n",
      "1023: accuracy:0.91 loss: 21.4389\n",
      "1024: accuracy:0.89 loss: 29.8103\n",
      "1025: accuracy:0.87 loss: 46.5755\n",
      "1026: accuracy:0.88 loss: 50.5748\n",
      "1027: accuracy:0.9 loss: 24.6648\n",
      "1028: accuracy:0.93 loss: 29.136\n",
      "1029: accuracy:0.93 loss: 39.6706\n",
      "1030: accuracy:0.89 loss: 41.3631\n",
      "1031: accuracy:0.91 loss: 30.9555\n",
      "1032: accuracy:0.92 loss: 40.2408\n",
      "1033: accuracy:0.93 loss: 24.2645\n",
      "1034: accuracy:0.87 loss: 39.4617\n",
      "1035: accuracy:0.91 loss: 24.8705\n",
      "1036: accuracy:0.89 loss: 39.9604\n",
      "1037: accuracy:0.91 loss: 27.746\n",
      "1038: accuracy:0.95 loss: 21.9356\n",
      "1039: accuracy:0.91 loss: 34.674\n",
      "1040: accuracy:0.96 loss: 15.1388\n",
      "1041: accuracy:0.89 loss: 39.4173\n",
      "1042: accuracy:0.94 loss: 24.1991\n",
      "1043: accuracy:0.89 loss: 40.0845\n",
      "1044: accuracy:0.93 loss: 23.4274\n",
      "1045: accuracy:0.89 loss: 37.5765\n",
      "1046: accuracy:0.91 loss: 27.4827\n",
      "1047: accuracy:0.9 loss: 27.0736\n",
      "1048: accuracy:0.98 loss: 11.5685\n",
      "1049: accuracy:0.88 loss: 51.1683\n",
      "1050: accuracy:0.89 loss: 41.0357\n",
      "1051: accuracy:0.94 loss: 22.8547\n",
      "1052: accuracy:0.96 loss: 15.8377\n",
      "1053: accuracy:0.92 loss: 22.7733\n",
      "1054: accuracy:0.85 loss: 51.2173\n",
      "1055: accuracy:0.95 loss: 26.8583\n",
      "1056: accuracy:0.92 loss: 34.202\n",
      "1057: accuracy:0.92 loss: 24.024\n",
      "1058: accuracy:0.91 loss: 34.627\n",
      "1059: accuracy:0.92 loss: 33.5091\n",
      "1060: accuracy:0.9 loss: 24.8019\n",
      "1061: accuracy:0.9 loss: 38.9514\n",
      "1062: accuracy:0.89 loss: 48.2192\n",
      "1063: accuracy:0.93 loss: 22.5796\n",
      "1064: accuracy:0.95 loss: 18.6293\n",
      "1065: accuracy:0.89 loss: 40.3794\n",
      "1066: accuracy:0.89 loss: 39.4714\n",
      "1067: accuracy:0.97 loss: 13.7412\n",
      "1068: accuracy:0.9 loss: 35.6201\n",
      "1069: accuracy:0.94 loss: 29.8982\n",
      "1070: accuracy:0.93 loss: 30.2422\n",
      "1071: accuracy:0.91 loss: 28.3143\n",
      "1072: accuracy:0.93 loss: 26.9361\n",
      "1073: accuracy:0.92 loss: 29.4911\n",
      "1074: accuracy:0.91 loss: 26.507\n",
      "1075: accuracy:0.93 loss: 19.43\n",
      "1076: accuracy:0.88 loss: 27.7981\n",
      "1077: accuracy:0.88 loss: 45.2844\n",
      "1078: accuracy:0.92 loss: 26.4997\n",
      "1079: accuracy:0.89 loss: 33.347\n",
      "1080: accuracy:0.99 loss: 11.6175\n",
      "1081: accuracy:0.9 loss: 39.8391\n",
      "1082: accuracy:0.9 loss: 28.4007\n",
      "1083: accuracy:0.9 loss: 31.1914\n",
      "1084: accuracy:0.95 loss: 18.199\n",
      "1085: accuracy:0.96 loss: 16.7162\n",
      "1086: accuracy:0.96 loss: 32.8924\n",
      "1087: accuracy:0.86 loss: 55.9958\n",
      "1088: accuracy:0.91 loss: 45.0458\n",
      "1089: accuracy:0.9 loss: 43.484\n",
      "1090: accuracy:0.92 loss: 21.7207\n",
      "1091: accuracy:0.93 loss: 17.5354\n",
      "1092: accuracy:0.94 loss: 15.7255\n",
      "1093: accuracy:0.88 loss: 33.7073\n",
      "1094: accuracy:0.97 loss: 20.0484\n",
      "1095: accuracy:0.85 loss: 39.9975\n",
      "1096: accuracy:0.92 loss: 41.4493\n",
      "1097: accuracy:0.91 loss: 30.1262\n",
      "1098: accuracy:0.95 loss: 20.0732\n",
      "1099: accuracy:0.92 loss: 29.4427\n",
      "1100: accuracy:0.92 loss: 27.1396\n",
      "1101: accuracy:0.96 loss: 15.9034\n",
      "1102: accuracy:0.94 loss: 26.7506\n",
      "1103: accuracy:0.87 loss: 36.9839\n",
      "1104: accuracy:0.93 loss: 23.5806\n",
      "1105: accuracy:0.97 loss: 15.6734\n",
      "1106: accuracy:0.97 loss: 16.77\n",
      "1107: accuracy:0.9 loss: 29.3535\n",
      "1108: accuracy:0.9 loss: 35.7672\n",
      "1109: accuracy:0.9 loss: 28.3267\n",
      "1110: accuracy:0.86 loss: 31.2648\n",
      "1111: accuracy:0.92 loss: 32.7977\n",
      "1112: accuracy:0.94 loss: 19.283\n",
      "1113: accuracy:0.93 loss: 21.9409\n",
      "1114: accuracy:0.9 loss: 29.0528\n",
      "1115: accuracy:0.91 loss: 35.2681\n",
      "1116: accuracy:0.9 loss: 24.9501\n",
      "1117: accuracy:0.94 loss: 23.9435\n",
      "1118: accuracy:0.92 loss: 24.9326\n",
      "1119: accuracy:0.93 loss: 26.1176\n",
      "1120: accuracy:0.94 loss: 20.3393\n",
      "1121: accuracy:0.93 loss: 23.9321\n",
      "1122: accuracy:0.91 loss: 44.2584\n",
      "1123: accuracy:0.94 loss: 32.9397\n",
      "1124: accuracy:0.94 loss: 30.6986\n",
      "1125: accuracy:0.9 loss: 33.0605\n",
      "1126: accuracy:0.9 loss: 31.6078\n",
      "1127: accuracy:0.91 loss: 24.1392\n",
      "1128: accuracy:0.91 loss: 31.3824\n",
      "1129: accuracy:0.95 loss: 19.1738\n",
      "1130: accuracy:0.87 loss: 48.4351\n",
      "1131: accuracy:0.91 loss: 32.4622\n",
      "1132: accuracy:0.92 loss: 24.5357\n",
      "1133: accuracy:0.89 loss: 41.9908\n",
      "1134: accuracy:0.93 loss: 26.0868\n",
      "1135: accuracy:0.92 loss: 20.8878\n",
      "1136: accuracy:0.99 loss: 7.8588\n",
      "1137: accuracy:0.9 loss: 26.8921\n",
      "1138: accuracy:0.89 loss: 34.9453\n",
      "1139: accuracy:0.86 loss: 34.3387\n",
      "1140: accuracy:0.92 loss: 31.2866\n",
      "1141: accuracy:0.93 loss: 28.2585\n",
      "1142: accuracy:0.92 loss: 20.1634\n",
      "1143: accuracy:0.9 loss: 47.7643\n",
      "1144: accuracy:0.94 loss: 48.5662\n",
      "1145: accuracy:0.91 loss: 39.3978\n",
      "1146: accuracy:0.97 loss: 18.0418\n",
      "1147: accuracy:0.9 loss: 33.1931\n",
      "1148: accuracy:0.93 loss: 23.5466\n",
      "1149: accuracy:0.93 loss: 20.2726\n",
      "1150: accuracy:0.92 loss: 22.0703\n",
      "1151: accuracy:0.88 loss: 33.293\n",
      "1152: accuracy:0.85 loss: 49.0151\n",
      "1153: accuracy:0.92 loss: 28.7953\n",
      "1154: accuracy:0.88 loss: 32.2542\n",
      "1155: accuracy:0.91 loss: 27.535\n",
      "1156: accuracy:0.93 loss: 25.0215\n",
      "1157: accuracy:0.91 loss: 31.1895\n",
      "1158: accuracy:0.94 loss: 26.0764\n",
      "1159: accuracy:0.92 loss: 35.9105\n",
      "1160: accuracy:0.95 loss: 18.6278\n",
      "1161: accuracy:0.88 loss: 34.0941\n",
      "1162: accuracy:0.89 loss: 41.2547\n",
      "1163: accuracy:0.96 loss: 16.5312\n",
      "1164: accuracy:0.92 loss: 29.2517\n",
      "1165: accuracy:0.93 loss: 27.7373\n",
      "1166: accuracy:0.91 loss: 27.9137\n",
      "1167: accuracy:0.9 loss: 35.7243\n",
      "1168: accuracy:0.93 loss: 34.4694\n",
      "1169: accuracy:0.92 loss: 34.9197\n",
      "1170: accuracy:0.93 loss: 23.3389\n",
      "1171: accuracy:0.93 loss: 22.6355\n",
      "1172: accuracy:0.94 loss: 18.3147\n",
      "1173: accuracy:0.94 loss: 20.8501\n",
      "1174: accuracy:0.91 loss: 25.1858\n",
      "1175: accuracy:0.96 loss: 21.7176\n",
      "1176: accuracy:0.92 loss: 31.2866\n",
      "1177: accuracy:0.94 loss: 18.681\n",
      "1178: accuracy:0.92 loss: 41.4681\n",
      "1179: accuracy:0.96 loss: 16.2971\n",
      "1180: accuracy:0.87 loss: 43.8166\n",
      "1181: accuracy:0.97 loss: 17.1894\n",
      "1182: accuracy:0.91 loss: 25.8739\n",
      "1183: accuracy:0.89 loss: 40.3736\n",
      "1184: accuracy:0.91 loss: 35.3293\n",
      "1185: accuracy:0.92 loss: 25.534\n",
      "1186: accuracy:0.96 loss: 15.3284\n",
      "1187: accuracy:0.94 loss: 17.2191\n",
      "1188: accuracy:0.93 loss: 23.9141\n",
      "1189: accuracy:0.93 loss: 28.5898\n",
      "1190: accuracy:0.87 loss: 40.0424\n",
      "1191: accuracy:0.89 loss: 43.5938\n",
      "1192: accuracy:0.96 loss: 17.6919\n",
      "1193: accuracy:0.97 loss: 15.6585\n",
      "1194: accuracy:0.93 loss: 18.9147\n",
      "1195: accuracy:0.9 loss: 30.4411\n",
      "1196: accuracy:0.94 loss: 17.2511\n",
      "1197: accuracy:0.94 loss: 27.3542\n",
      "1198: accuracy:0.9 loss: 43.5885\n",
      "1199: accuracy:0.92 loss: 31.6558\n",
      "1200: accuracy:0.94 loss: 17.634\n",
      "1201: accuracy:0.87 loss: 37.7646\n",
      "1202: accuracy:0.91 loss: 34.0459\n",
      "1203: accuracy:0.9 loss: 49.1233\n",
      "1204: accuracy:0.94 loss: 31.2488\n",
      "1205: accuracy:0.9 loss: 29.4145\n",
      "1206: accuracy:0.87 loss: 40.8543\n",
      "1207: accuracy:0.91 loss: 31.787\n",
      "1208: accuracy:0.95 loss: 24.5009\n",
      "1209: accuracy:0.9 loss: 32.905\n",
      "1210: accuracy:0.95 loss: 15.1735\n",
      "1211: accuracy:0.89 loss: 41.3106\n",
      "1212: accuracy:0.86 loss: 49.1934\n",
      "1213: accuracy:0.94 loss: 19.706\n",
      "1214: accuracy:0.93 loss: 33.6089\n",
      "1215: accuracy:0.91 loss: 32.6189\n",
      "1216: accuracy:0.98 loss: 15.2637\n",
      "1217: accuracy:0.89 loss: 35.3676\n",
      "1218: accuracy:0.9 loss: 32.9239\n",
      "1219: accuracy:0.88 loss: 49.8593\n",
      "1220: accuracy:0.9 loss: 22.9507\n",
      "1221: accuracy:0.95 loss: 19.9927\n",
      "1222: accuracy:0.92 loss: 26.2274\n",
      "1223: accuracy:0.89 loss: 35.2775\n",
      "1224: accuracy:0.93 loss: 24.6391\n",
      "1225: accuracy:0.94 loss: 26.1137\n",
      "1226: accuracy:0.95 loss: 14.1233\n",
      "1227: accuracy:0.92 loss: 27.9706\n",
      "1228: accuracy:0.92 loss: 25.7123\n",
      "1229: accuracy:0.89 loss: 41.8645\n",
      "1230: accuracy:0.93 loss: 38.9697\n",
      "1231: accuracy:0.94 loss: 21.6251\n",
      "1232: accuracy:0.85 loss: 49.0725\n",
      "1233: accuracy:0.92 loss: 26.4605\n",
      "1234: accuracy:0.94 loss: 29.6582\n",
      "1235: accuracy:0.94 loss: 28.4648\n",
      "1236: accuracy:0.92 loss: 22.7227\n",
      "1237: accuracy:0.91 loss: 28.8368\n",
      "1238: accuracy:0.93 loss: 37.9033\n",
      "1239: accuracy:0.91 loss: 33.7699\n",
      "1240: accuracy:0.93 loss: 24.7206\n",
      "1241: accuracy:0.93 loss: 23.8383\n",
      "1242: accuracy:0.96 loss: 17.6262\n",
      "1243: accuracy:0.92 loss: 27.726\n",
      "1244: accuracy:0.93 loss: 19.7687\n",
      "1245: accuracy:0.91 loss: 29.869\n",
      "1246: accuracy:0.91 loss: 26.0484\n",
      "1247: accuracy:0.89 loss: 28.9786\n",
      "1248: accuracy:0.93 loss: 30.8176\n",
      "1249: accuracy:0.9 loss: 30.9109\n",
      "1250: accuracy:0.92 loss: 38.2277\n",
      "1251: accuracy:0.87 loss: 55.0474\n",
      "1252: accuracy:0.96 loss: 19.3364\n",
      "1253: accuracy:0.92 loss: 23.0299\n",
      "1254: accuracy:0.93 loss: 24.9346\n",
      "1255: accuracy:0.93 loss: 18.6598\n",
      "1256: accuracy:0.92 loss: 33.4793\n",
      "1257: accuracy:0.96 loss: 29.2785\n",
      "1258: accuracy:0.87 loss: 35.7256\n",
      "1259: accuracy:0.88 loss: 36.9173\n",
      "1260: accuracy:0.9 loss: 35.2122\n",
      "1261: accuracy:0.9 loss: 34.9479\n",
      "1262: accuracy:0.88 loss: 33.7468\n",
      "1263: accuracy:0.91 loss: 30.7584\n",
      "1264: accuracy:0.96 loss: 24.0819\n",
      "1265: accuracy:0.88 loss: 29.2273\n",
      "1266: accuracy:0.97 loss: 15.6424\n",
      "1267: accuracy:0.95 loss: 23.7679\n",
      "1268: accuracy:0.87 loss: 47.0161\n",
      "1269: accuracy:0.87 loss: 36.863\n",
      "1270: accuracy:0.91 loss: 32.9608\n",
      "1271: accuracy:0.95 loss: 24.6423\n",
      "1272: accuracy:0.94 loss: 18.936\n",
      "1273: accuracy:0.9 loss: 28.7559\n",
      "1274: accuracy:0.92 loss: 28.4012\n",
      "1275: accuracy:0.9 loss: 29.9324\n",
      "1276: accuracy:0.94 loss: 26.7408\n",
      "1277: accuracy:0.9 loss: 35.087\n",
      "1278: accuracy:0.89 loss: 30.5791\n",
      "1279: accuracy:0.94 loss: 15.7504\n",
      "1280: accuracy:0.97 loss: 15.3151\n",
      "1281: accuracy:0.95 loss: 17.1104\n",
      "1282: accuracy:0.92 loss: 25.5649\n",
      "1283: accuracy:0.9 loss: 21.7437\n",
      "1284: accuracy:0.89 loss: 30.5101\n",
      "1285: accuracy:0.89 loss: 38.3314\n",
      "1286: accuracy:0.89 loss: 37.8445\n",
      "1287: accuracy:0.89 loss: 30.1486\n",
      "1288: accuracy:0.92 loss: 37.454\n",
      "1289: accuracy:0.96 loss: 21.5797\n",
      "1290: accuracy:0.93 loss: 23.7985\n",
      "1291: accuracy:0.88 loss: 40.8781\n",
      "1292: accuracy:0.91 loss: 29.8015\n",
      "1293: accuracy:0.94 loss: 21.8347\n",
      "1294: accuracy:0.95 loss: 17.306\n",
      "1295: accuracy:0.9 loss: 37.4691\n",
      "1296: accuracy:0.9 loss: 42.6101\n",
      "1297: accuracy:0.9 loss: 41.5713\n",
      "1298: accuracy:0.92 loss: 32.9155\n",
      "1299: accuracy:0.96 loss: 19.7359\n",
      "1300: accuracy:0.91 loss: 31.8853\n",
      "1301: accuracy:0.93 loss: 30.3154\n",
      "1302: accuracy:0.94 loss: 22.6714\n",
      "1303: accuracy:0.89 loss: 32.1645\n",
      "1304: accuracy:0.93 loss: 20.1023\n",
      "1305: accuracy:0.87 loss: 60.1518\n",
      "1306: accuracy:0.96 loss: 21.0289\n",
      "1307: accuracy:0.92 loss: 24.7689\n",
      "1308: accuracy:0.86 loss: 42.9592\n",
      "1309: accuracy:0.9 loss: 30.9658\n",
      "1310: accuracy:0.95 loss: 20.4282\n",
      "1311: accuracy:0.88 loss: 40.3577\n",
      "1312: accuracy:0.95 loss: 20.1474\n",
      "1313: accuracy:0.84 loss: 44.0093\n",
      "1314: accuracy:0.93 loss: 36.2943\n",
      "1315: accuracy:0.95 loss: 17.7067\n",
      "1316: accuracy:0.9 loss: 44.7029\n",
      "1317: accuracy:0.91 loss: 31.3023\n",
      "1318: accuracy:0.86 loss: 52.4281\n",
      "1319: accuracy:0.86 loss: 47.12\n",
      "1320: accuracy:0.89 loss: 43.0149\n",
      "1321: accuracy:0.9 loss: 38.3069\n",
      "1322: accuracy:0.95 loss: 17.2774\n",
      "1323: accuracy:0.91 loss: 34.1917\n",
      "1324: accuracy:0.93 loss: 28.9902\n",
      "1325: accuracy:0.9 loss: 37.7005\n",
      "1326: accuracy:0.86 loss: 37.4717\n",
      "1327: accuracy:0.93 loss: 20.4112\n",
      "1328: accuracy:0.92 loss: 28.0943\n",
      "1329: accuracy:0.93 loss: 23.1658\n",
      "1330: accuracy:0.91 loss: 29.1862\n",
      "1331: accuracy:0.92 loss: 23.8619\n",
      "1332: accuracy:0.9 loss: 36.4417\n",
      "1333: accuracy:0.89 loss: 28.1659\n",
      "1334: accuracy:0.9 loss: 37.3964\n",
      "1335: accuracy:0.93 loss: 18.9168\n",
      "1336: accuracy:0.89 loss: 28.3898\n",
      "1337: accuracy:0.93 loss: 25.8268\n",
      "1338: accuracy:0.96 loss: 36.3416\n",
      "1339: accuracy:0.96 loss: 13.3301\n",
      "1340: accuracy:0.91 loss: 29.9829\n",
      "1341: accuracy:0.89 loss: 41.7646\n",
      "1342: accuracy:0.94 loss: 15.3201\n",
      "1343: accuracy:0.91 loss: 32.0077\n",
      "1344: accuracy:0.88 loss: 30.346\n",
      "1345: accuracy:0.89 loss: 26.1024\n",
      "1346: accuracy:0.93 loss: 32.6497\n",
      "1347: accuracy:0.93 loss: 31.0679\n",
      "1348: accuracy:0.9 loss: 38.3023\n",
      "1349: accuracy:0.91 loss: 35.81\n",
      "1350: accuracy:0.89 loss: 45.7487\n",
      "1351: accuracy:0.9 loss: 41.2027\n",
      "1352: accuracy:0.92 loss: 29.8613\n",
      "1353: accuracy:0.9 loss: 25.4806\n",
      "1354: accuracy:0.93 loss: 21.5059\n",
      "1355: accuracy:0.91 loss: 27.7186\n",
      "1356: accuracy:0.94 loss: 17.7144\n",
      "1357: accuracy:0.91 loss: 25.2315\n",
      "1358: accuracy:0.87 loss: 33.9524\n",
      "1359: accuracy:0.93 loss: 28.9012\n",
      "1360: accuracy:0.97 loss: 17.5604\n",
      "1361: accuracy:0.93 loss: 21.82\n",
      "1362: accuracy:0.94 loss: 22.7814\n",
      "1363: accuracy:0.94 loss: 19.664\n",
      "1364: accuracy:0.89 loss: 33.2767\n",
      "1365: accuracy:0.93 loss: 27.1152\n",
      "1366: accuracy:0.92 loss: 24.5961\n",
      "1367: accuracy:0.88 loss: 39.4327\n",
      "1368: accuracy:0.97 loss: 12.892\n",
      "1369: accuracy:0.94 loss: 21.2568\n",
      "1370: accuracy:0.93 loss: 27.319\n",
      "1371: accuracy:0.91 loss: 34.5329\n",
      "1372: accuracy:0.97 loss: 18.6309\n",
      "1373: accuracy:0.92 loss: 41.6865\n",
      "1374: accuracy:0.93 loss: 25.9312\n",
      "1375: accuracy:0.93 loss: 20.985\n",
      "1376: accuracy:0.92 loss: 29.4034\n",
      "1377: accuracy:0.87 loss: 39.5194\n",
      "1378: accuracy:0.95 loss: 19.9765\n",
      "1379: accuracy:0.89 loss: 30.0513\n",
      "1380: accuracy:0.9 loss: 25.1888\n",
      "1381: accuracy:0.97 loss: 14.7525\n",
      "1382: accuracy:0.95 loss: 19.6154\n",
      "1383: accuracy:0.92 loss: 27.0168\n",
      "1384: accuracy:0.94 loss: 25.1813\n",
      "1385: accuracy:0.92 loss: 33.4937\n",
      "1386: accuracy:0.94 loss: 31.4312\n",
      "1387: accuracy:0.93 loss: 37.4913\n",
      "1388: accuracy:0.89 loss: 42.3147\n",
      "1389: accuracy:0.92 loss: 24.2171\n",
      "1390: accuracy:0.93 loss: 27.6991\n",
      "1391: accuracy:0.91 loss: 44.3459\n",
      "1392: accuracy:0.91 loss: 35.9605\n",
      "1393: accuracy:0.92 loss: 38.2346\n",
      "1394: accuracy:0.9 loss: 32.7679\n",
      "1395: accuracy:0.94 loss: 18.6733\n",
      "1396: accuracy:0.87 loss: 44.266\n",
      "1397: accuracy:0.94 loss: 23.7737\n",
      "1398: accuracy:0.91 loss: 24.6801\n",
      "1399: accuracy:0.93 loss: 24.1886\n",
      "1400: accuracy:0.95 loss: 24.0534\n",
      "1401: accuracy:0.93 loss: 26.229\n",
      "1402: accuracy:0.93 loss: 26.664\n",
      "1403: accuracy:0.9 loss: 29.625\n",
      "1404: accuracy:0.92 loss: 30.8064\n",
      "1405: accuracy:0.92 loss: 23.1964\n",
      "1406: accuracy:0.93 loss: 39.4268\n",
      "1407: accuracy:0.95 loss: 15.5168\n",
      "1408: accuracy:0.94 loss: 19.6901\n",
      "1409: accuracy:0.94 loss: 14.0143\n",
      "1410: accuracy:0.93 loss: 28.3213\n",
      "1411: accuracy:0.91 loss: 31.9732\n",
      "1412: accuracy:0.91 loss: 25.4348\n",
      "1413: accuracy:0.89 loss: 46.9159\n",
      "1414: accuracy:0.89 loss: 25.3313\n",
      "1415: accuracy:0.92 loss: 31.8276\n",
      "1416: accuracy:0.9 loss: 46.226\n",
      "1417: accuracy:0.88 loss: 36.8061\n",
      "1418: accuracy:0.94 loss: 25.753\n",
      "1419: accuracy:0.94 loss: 24.8276\n",
      "1420: accuracy:0.9 loss: 36.0178\n",
      "1421: accuracy:0.92 loss: 28.7472\n",
      "1422: accuracy:0.87 loss: 41.4195\n",
      "1423: accuracy:0.95 loss: 18.004\n",
      "1424: accuracy:0.9 loss: 29.9103\n",
      "1425: accuracy:0.93 loss: 25.5126\n",
      "1426: accuracy:0.93 loss: 21.8224\n",
      "1427: accuracy:0.93 loss: 33.642\n",
      "1428: accuracy:0.9 loss: 33.4257\n",
      "1429: accuracy:0.94 loss: 24.5011\n",
      "1430: accuracy:0.95 loss: 22.0734\n",
      "1431: accuracy:0.93 loss: 22.9653\n",
      "1432: accuracy:0.92 loss: 42.0292\n",
      "1433: accuracy:0.88 loss: 32.7782\n",
      "1434: accuracy:0.95 loss: 24.8811\n",
      "1435: accuracy:0.91 loss: 31.8585\n",
      "1436: accuracy:0.92 loss: 28.4342\n",
      "1437: accuracy:0.94 loss: 23.5597\n",
      "1438: accuracy:0.9 loss: 44.5784\n",
      "1439: accuracy:0.94 loss: 21.8286\n",
      "1440: accuracy:0.88 loss: 39.8266\n",
      "1441: accuracy:0.88 loss: 28.4486\n",
      "1442: accuracy:0.89 loss: 30.4197\n",
      "1443: accuracy:0.92 loss: 23.2963\n",
      "1444: accuracy:0.9 loss: 28.3207\n",
      "1445: accuracy:0.92 loss: 22.9921\n",
      "1446: accuracy:0.96 loss: 15.2628\n",
      "1447: accuracy:0.93 loss: 20.8085\n",
      "1448: accuracy:0.94 loss: 17.8444\n",
      "1449: accuracy:0.9 loss: 30.7124\n",
      "1450: accuracy:0.88 loss: 41.3877\n",
      "1451: accuracy:0.86 loss: 52.1448\n",
      "1452: accuracy:0.96 loss: 19.514\n",
      "1453: accuracy:0.9 loss: 26.5599\n",
      "1454: accuracy:0.93 loss: 26.2887\n",
      "1455: accuracy:0.89 loss: 30.4888\n",
      "1456: accuracy:0.94 loss: 17.8985\n",
      "1457: accuracy:0.93 loss: 31.4399\n",
      "1458: accuracy:0.9 loss: 34.5524\n",
      "1459: accuracy:0.92 loss: 30.4351\n",
      "1460: accuracy:0.92 loss: 35.7436\n",
      "1461: accuracy:0.91 loss: 28.287\n",
      "1462: accuracy:0.9 loss: 31.707\n",
      "1463: accuracy:0.9 loss: 29.5065\n",
      "1464: accuracy:0.94 loss: 33.5195\n",
      "1465: accuracy:0.9 loss: 34.9242\n",
      "1466: accuracy:0.94 loss: 17.7156\n",
      "1467: accuracy:0.93 loss: 19.7448\n",
      "1468: accuracy:0.91 loss: 32.9053\n",
      "1469: accuracy:0.9 loss: 31.0906\n",
      "1470: accuracy:0.94 loss: 17.5034\n",
      "1471: accuracy:0.89 loss: 42.9233\n",
      "1472: accuracy:0.91 loss: 30.0021\n",
      "1473: accuracy:0.93 loss: 25.7059\n",
      "1474: accuracy:0.84 loss: 42.6937\n",
      "1475: accuracy:0.92 loss: 31.4236\n",
      "1476: accuracy:0.91 loss: 28.3653\n",
      "1477: accuracy:0.88 loss: 30.7222\n",
      "1478: accuracy:0.89 loss: 22.5161\n",
      "1479: accuracy:0.93 loss: 23.5678\n",
      "1480: accuracy:0.88 loss: 37.2413\n",
      "1481: accuracy:0.96 loss: 13.7245\n",
      "1482: accuracy:0.95 loss: 22.5394\n",
      "1483: accuracy:0.93 loss: 31.7407\n",
      "1484: accuracy:0.93 loss: 22.6678\n",
      "1485: accuracy:0.9 loss: 24.8508\n",
      "1486: accuracy:0.92 loss: 26.4531\n",
      "1487: accuracy:0.88 loss: 45.4664\n",
      "1488: accuracy:0.93 loss: 35.2038\n",
      "1489: accuracy:0.91 loss: 25.8521\n",
      "1490: accuracy:0.91 loss: 38.3607\n",
      "1491: accuracy:0.89 loss: 35.6842\n",
      "1492: accuracy:0.85 loss: 49.2832\n",
      "1493: accuracy:0.93 loss: 25.3605\n",
      "1494: accuracy:0.97 loss: 17.6796\n",
      "1495: accuracy:0.92 loss: 26.6257\n",
      "1496: accuracy:0.91 loss: 33.7335\n",
      "1497: accuracy:0.93 loss: 19.9789\n",
      "1498: accuracy:0.89 loss: 37.0712\n",
      "1499: accuracy:0.93 loss: 25.1086\n",
      "1500: accuracy:0.94 loss: 30.4087\n",
      "1501: accuracy:0.94 loss: 26.123\n",
      "1502: accuracy:0.87 loss: 48.3811\n",
      "1503: accuracy:0.92 loss: 30.4682\n",
      "1504: accuracy:0.95 loss: 17.493\n",
      "1505: accuracy:0.88 loss: 37.6063\n",
      "1506: accuracy:0.92 loss: 24.6712\n",
      "1507: accuracy:0.9 loss: 29.3894\n",
      "1508: accuracy:0.9 loss: 31.6096\n",
      "1509: accuracy:0.89 loss: 28.9795\n",
      "1510: accuracy:0.94 loss: 19.2128\n",
      "1511: accuracy:0.9 loss: 25.0331\n",
      "1512: accuracy:0.83 loss: 40.7413\n",
      "1513: accuracy:0.88 loss: 43.3173\n",
      "1514: accuracy:0.9 loss: 34.509\n",
      "1515: accuracy:0.91 loss: 28.329\n",
      "1516: accuracy:0.95 loss: 27.577\n",
      "1517: accuracy:0.92 loss: 32.9194\n",
      "1518: accuracy:0.94 loss: 40.9021\n",
      "1519: accuracy:0.89 loss: 30.0632\n",
      "1520: accuracy:0.92 loss: 21.2852\n",
      "1521: accuracy:0.93 loss: 29.7521\n",
      "1522: accuracy:0.91 loss: 28.7399\n",
      "1523: accuracy:0.93 loss: 29.9457\n",
      "1524: accuracy:0.92 loss: 20.544\n",
      "1525: accuracy:0.96 loss: 17.8162\n",
      "1526: accuracy:0.89 loss: 25.5734\n",
      "1527: accuracy:0.95 loss: 16.9887\n",
      "1528: accuracy:0.92 loss: 24.8898\n",
      "1529: accuracy:0.93 loss: 28.0608\n",
      "1530: accuracy:0.94 loss: 31.1479\n",
      "1531: accuracy:0.95 loss: 28.9059\n",
      "1532: accuracy:0.94 loss: 18.782\n",
      "1533: accuracy:0.93 loss: 22.2738\n",
      "1534: accuracy:0.95 loss: 19.6642\n",
      "1535: accuracy:0.92 loss: 32.309\n",
      "1536: accuracy:0.92 loss: 24.8488\n",
      "1537: accuracy:0.9 loss: 35.9204\n",
      "1538: accuracy:0.96 loss: 12.3646\n",
      "1539: accuracy:0.93 loss: 28.057\n",
      "1540: accuracy:0.93 loss: 17.3584\n",
      "1541: accuracy:0.96 loss: 21.1601\n",
      "1542: accuracy:0.93 loss: 24.1171\n",
      "1543: accuracy:0.9 loss: 29.791\n",
      "1544: accuracy:0.9 loss: 31.4237\n",
      "1545: accuracy:0.91 loss: 31.4383\n",
      "1546: accuracy:0.9 loss: 28.0573\n",
      "1547: accuracy:0.96 loss: 24.9542\n",
      "1548: accuracy:0.9 loss: 26.347\n",
      "1549: accuracy:0.89 loss: 42.2032\n",
      "1550: accuracy:0.91 loss: 32.9604\n",
      "1551: accuracy:0.97 loss: 14.3181\n",
      "1552: accuracy:0.94 loss: 21.7863\n",
      "1553: accuracy:0.96 loss: 28.8884\n",
      "1554: accuracy:0.88 loss: 32.8199\n",
      "1555: accuracy:0.92 loss: 29.3422\n",
      "1556: accuracy:0.89 loss: 37.7555\n",
      "1557: accuracy:0.89 loss: 32.6293\n",
      "1558: accuracy:0.89 loss: 37.4283\n",
      "1559: accuracy:0.96 loss: 18.6934\n",
      "1560: accuracy:0.92 loss: 27.3207\n",
      "1561: accuracy:0.93 loss: 28.303\n",
      "1562: accuracy:0.95 loss: 23.087\n",
      "1563: accuracy:0.97 loss: 15.7567\n",
      "1564: accuracy:0.91 loss: 43.4687\n",
      "1565: accuracy:0.88 loss: 30.2457\n",
      "1566: accuracy:0.94 loss: 20.0816\n",
      "1567: accuracy:0.95 loss: 15.6903\n",
      "1568: accuracy:0.93 loss: 26.1981\n",
      "1569: accuracy:0.94 loss: 23.3174\n",
      "1570: accuracy:0.89 loss: 31.3214\n",
      "1571: accuracy:0.96 loss: 20.9524\n",
      "1572: accuracy:0.89 loss: 38.6694\n",
      "1573: accuracy:0.92 loss: 23.4188\n",
      "1574: accuracy:0.91 loss: 30.8099\n",
      "1575: accuracy:0.91 loss: 24.6935\n",
      "1576: accuracy:0.88 loss: 35.5825\n",
      "1577: accuracy:0.89 loss: 47.0552\n",
      "1578: accuracy:0.96 loss: 18.0275\n",
      "1579: accuracy:0.95 loss: 17.8682\n",
      "1580: accuracy:0.91 loss: 33.2975\n",
      "1581: accuracy:0.94 loss: 27.7313\n",
      "1582: accuracy:0.93 loss: 30.3788\n",
      "1583: accuracy:0.94 loss: 19.0052\n",
      "1584: accuracy:0.93 loss: 20.4078\n",
      "1585: accuracy:0.89 loss: 58.8169\n",
      "1586: accuracy:0.91 loss: 28.131\n",
      "1587: accuracy:0.93 loss: 28.6029\n",
      "1588: accuracy:0.97 loss: 15.9574\n",
      "1589: accuracy:0.89 loss: 38.2895\n",
      "1590: accuracy:0.95 loss: 29.5688\n",
      "1591: accuracy:0.95 loss: 25.9483\n",
      "1592: accuracy:0.92 loss: 24.9817\n",
      "1593: accuracy:0.94 loss: 26.2831\n",
      "1594: accuracy:0.92 loss: 19.475\n",
      "1595: accuracy:0.92 loss: 23.5613\n",
      "1596: accuracy:0.93 loss: 23.6242\n",
      "1597: accuracy:0.91 loss: 23.7304\n",
      "1598: accuracy:0.89 loss: 33.1214\n",
      "1599: accuracy:0.94 loss: 34.0831\n",
      "1600: accuracy:0.93 loss: 49.0731\n",
      "1601: accuracy:0.93 loss: 36.5344\n",
      "1602: accuracy:0.93 loss: 22.2396\n",
      "1603: accuracy:0.89 loss: 37.2055\n",
      "1604: accuracy:0.95 loss: 20.2726\n",
      "1605: accuracy:0.95 loss: 17.8048\n",
      "1606: accuracy:0.91 loss: 24.7942\n",
      "1607: accuracy:0.94 loss: 18.0727\n",
      "1608: accuracy:0.93 loss: 19.3247\n",
      "1609: accuracy:0.88 loss: 39.0273\n",
      "1610: accuracy:0.95 loss: 25.5374\n",
      "1611: accuracy:0.88 loss: 35.52\n",
      "1612: accuracy:0.94 loss: 24.9785\n",
      "1613: accuracy:0.89 loss: 40.7019\n",
      "1614: accuracy:0.95 loss: 18.4232\n",
      "1615: accuracy:0.91 loss: 28.662\n",
      "1616: accuracy:0.93 loss: 30.6312\n",
      "1617: accuracy:0.94 loss: 25.6784\n",
      "1618: accuracy:0.95 loss: 25.5898\n",
      "1619: accuracy:0.9 loss: 27.9065\n",
      "1620: accuracy:0.92 loss: 30.0666\n",
      "1621: accuracy:0.93 loss: 23.1464\n",
      "1622: accuracy:0.95 loss: 23.1753\n",
      "1623: accuracy:0.89 loss: 32.0497\n",
      "1624: accuracy:0.96 loss: 16.8217\n",
      "1625: accuracy:0.92 loss: 33.4833\n",
      "1626: accuracy:0.87 loss: 34.2205\n",
      "1627: accuracy:0.93 loss: 25.9523\n",
      "1628: accuracy:0.93 loss: 22.2109\n",
      "1629: accuracy:0.94 loss: 30.7937\n",
      "1630: accuracy:0.96 loss: 13.9333\n",
      "1631: accuracy:0.88 loss: 35.6294\n",
      "1632: accuracy:0.9 loss: 34.9762\n",
      "1633: accuracy:0.91 loss: 20.4208\n",
      "1634: accuracy:0.88 loss: 56.916\n",
      "1635: accuracy:0.93 loss: 23.8882\n",
      "1636: accuracy:0.94 loss: 17.6374\n",
      "1637: accuracy:0.91 loss: 26.4337\n",
      "1638: accuracy:0.95 loss: 22.411\n",
      "1639: accuracy:0.84 loss: 47.4253\n",
      "1640: accuracy:0.88 loss: 29.9223\n",
      "1641: accuracy:0.93 loss: 25.6209\n",
      "1642: accuracy:0.92 loss: 39.4603\n",
      "1643: accuracy:0.91 loss: 32.6475\n",
      "1644: accuracy:0.94 loss: 28.9282\n",
      "1645: accuracy:0.95 loss: 21.3663\n",
      "1646: accuracy:0.92 loss: 32.3843\n",
      "1647: accuracy:0.91 loss: 30.4951\n",
      "1648: accuracy:0.89 loss: 36.6354\n",
      "1649: accuracy:0.87 loss: 45.1994\n",
      "1650: accuracy:0.95 loss: 26.3257\n",
      "1651: accuracy:0.89 loss: 28.3749\n",
      "1652: accuracy:0.9 loss: 53.3702\n",
      "1653: accuracy:0.95 loss: 25.0464\n",
      "1654: accuracy:0.92 loss: 30.1545\n",
      "1655: accuracy:0.95 loss: 16.7454\n",
      "1656: accuracy:0.88 loss: 43.624\n",
      "1657: accuracy:0.9 loss: 36.7338\n",
      "1658: accuracy:0.94 loss: 24.5373\n",
      "1659: accuracy:0.94 loss: 22.6031\n",
      "1660: accuracy:0.91 loss: 26.4691\n",
      "1661: accuracy:0.92 loss: 30.8039\n",
      "1662: accuracy:0.92 loss: 27.5281\n",
      "1663: accuracy:0.93 loss: 32.017\n",
      "1664: accuracy:0.94 loss: 17.2721\n",
      "1665: accuracy:0.91 loss: 25.2174\n",
      "1666: accuracy:0.89 loss: 47.241\n",
      "1667: accuracy:0.94 loss: 23.0551\n",
      "1668: accuracy:0.94 loss: 25.1318\n",
      "1669: accuracy:0.89 loss: 25.7532\n",
      "1670: accuracy:0.9 loss: 38.7686\n",
      "1671: accuracy:0.94 loss: 21.3619\n",
      "1672: accuracy:0.95 loss: 26.0223\n",
      "1673: accuracy:0.91 loss: 30.7931\n",
      "1674: accuracy:0.96 loss: 14.1015\n",
      "1675: accuracy:0.94 loss: 22.0658\n",
      "1676: accuracy:0.92 loss: 28.275\n",
      "1677: accuracy:0.96 loss: 18.1718\n",
      "1678: accuracy:0.91 loss: 24.5359\n",
      "1679: accuracy:0.85 loss: 40.8028\n",
      "1680: accuracy:0.92 loss: 27.1809\n",
      "1681: accuracy:0.95 loss: 18.7137\n",
      "1682: accuracy:0.88 loss: 37.7357\n",
      "1683: accuracy:0.92 loss: 22.1799\n",
      "1684: accuracy:0.9 loss: 37.9796\n",
      "1685: accuracy:0.95 loss: 24.3757\n",
      "1686: accuracy:0.89 loss: 27.6878\n",
      "1687: accuracy:0.88 loss: 44.1411\n",
      "1688: accuracy:0.91 loss: 30.0734\n",
      "1689: accuracy:0.94 loss: 18.5242\n",
      "1690: accuracy:0.97 loss: 15.0033\n",
      "1691: accuracy:0.9 loss: 32.691\n",
      "1692: accuracy:0.95 loss: 25.1794\n",
      "1693: accuracy:0.96 loss: 19.8317\n",
      "1694: accuracy:0.89 loss: 28.0986\n",
      "1695: accuracy:0.93 loss: 28.8734\n",
      "1696: accuracy:0.87 loss: 44.5976\n",
      "1697: accuracy:0.94 loss: 19.8525\n",
      "1698: accuracy:0.95 loss: 21.2004\n",
      "1699: accuracy:0.94 loss: 28.1771\n",
      "1700: accuracy:0.92 loss: 20.7331\n",
      "1701: accuracy:0.95 loss: 24.2025\n",
      "1702: accuracy:0.9 loss: 24.4222\n",
      "1703: accuracy:0.94 loss: 16.9987\n",
      "1704: accuracy:0.92 loss: 43.897\n",
      "1705: accuracy:0.86 loss: 44.7771\n",
      "1706: accuracy:0.94 loss: 29.0008\n",
      "1707: accuracy:0.9 loss: 38.7078\n",
      "1708: accuracy:0.93 loss: 33.2775\n",
      "1709: accuracy:0.9 loss: 37.7187\n",
      "1710: accuracy:0.96 loss: 12.2345\n",
      "1711: accuracy:0.9 loss: 25.8199\n",
      "1712: accuracy:0.96 loss: 17.4782\n",
      "1713: accuracy:0.93 loss: 22.8051\n",
      "1714: accuracy:0.95 loss: 21.2652\n",
      "1715: accuracy:0.89 loss: 40.8785\n",
      "1716: accuracy:0.99 loss: 12.732\n",
      "1717: accuracy:0.93 loss: 27.158\n",
      "1718: accuracy:0.92 loss: 20.2934\n",
      "1719: accuracy:0.95 loss: 17.5117\n",
      "1720: accuracy:0.91 loss: 32.8326\n",
      "1721: accuracy:0.95 loss: 22.1015\n",
      "1722: accuracy:0.91 loss: 32.0807\n",
      "1723: accuracy:0.88 loss: 34.698\n",
      "1724: accuracy:0.94 loss: 21.5019\n",
      "1725: accuracy:0.93 loss: 21.4522\n",
      "1726: accuracy:0.88 loss: 45.038\n",
      "1727: accuracy:0.97 loss: 9.36568\n",
      "1728: accuracy:0.89 loss: 43.8338\n",
      "1729: accuracy:0.91 loss: 24.7738\n",
      "1730: accuracy:0.91 loss: 31.9413\n",
      "1731: accuracy:0.88 loss: 28.3209\n",
      "1732: accuracy:0.95 loss: 18.3266\n",
      "1733: accuracy:0.95 loss: 27.5071\n",
      "1734: accuracy:0.9 loss: 40.527\n",
      "1735: accuracy:0.95 loss: 24.6116\n",
      "1736: accuracy:0.93 loss: 31.0036\n",
      "1737: accuracy:0.89 loss: 39.6418\n",
      "1738: accuracy:0.94 loss: 27.2949\n",
      "1739: accuracy:0.93 loss: 27.7011\n",
      "1740: accuracy:0.89 loss: 31.1965\n",
      "1741: accuracy:0.89 loss: 39.7687\n",
      "1742: accuracy:0.93 loss: 28.338\n",
      "1743: accuracy:0.88 loss: 38.2818\n",
      "1744: accuracy:0.95 loss: 33.7344\n",
      "1745: accuracy:0.9 loss: 25.4528\n",
      "1746: accuracy:0.88 loss: 31.9438\n",
      "1747: accuracy:0.94 loss: 22.0312\n",
      "1748: accuracy:0.88 loss: 35.86\n",
      "1749: accuracy:0.94 loss: 21.3378\n",
      "1750: accuracy:0.96 loss: 28.6202\n",
      "1751: accuracy:0.95 loss: 30.8691\n",
      "1752: accuracy:0.97 loss: 14.3889\n",
      "1753: accuracy:0.91 loss: 35.1154\n",
      "1754: accuracy:0.9 loss: 25.3162\n",
      "1755: accuracy:0.93 loss: 25.0242\n",
      "1756: accuracy:0.89 loss: 25.1721\n",
      "1757: accuracy:0.93 loss: 52.602\n",
      "1758: accuracy:0.96 loss: 15.0459\n",
      "1759: accuracy:0.93 loss: 21.7912\n",
      "1760: accuracy:0.9 loss: 29.314\n",
      "1761: accuracy:0.91 loss: 27.8944\n",
      "1762: accuracy:0.97 loss: 13.8763\n",
      "1763: accuracy:0.97 loss: 19.0663\n",
      "1764: accuracy:0.94 loss: 25.2807\n",
      "1765: accuracy:0.95 loss: 18.9312\n",
      "1766: accuracy:0.89 loss: 30.487\n",
      "1767: accuracy:0.85 loss: 45.7026\n",
      "1768: accuracy:0.88 loss: 38.4295\n",
      "1769: accuracy:0.9 loss: 41.4401\n",
      "1770: accuracy:0.89 loss: 53.586\n",
      "1771: accuracy:0.91 loss: 27.5914\n",
      "1772: accuracy:0.93 loss: 28.8956\n",
      "1773: accuracy:0.91 loss: 30.2109\n",
      "1774: accuracy:0.93 loss: 19.5493\n",
      "1775: accuracy:0.97 loss: 13.5968\n",
      "1776: accuracy:0.92 loss: 24.9645\n",
      "1777: accuracy:0.89 loss: 32.1582\n",
      "1778: accuracy:0.95 loss: 20.9831\n",
      "1779: accuracy:0.89 loss: 45.5207\n",
      "1780: accuracy:0.94 loss: 19.1926\n",
      "1781: accuracy:0.9 loss: 26.073\n",
      "1782: accuracy:0.98 loss: 13.6779\n",
      "1783: accuracy:0.91 loss: 26.7718\n",
      "1784: accuracy:0.91 loss: 22.1436\n",
      "1785: accuracy:0.94 loss: 29.8193\n",
      "1786: accuracy:0.94 loss: 22.6472\n",
      "1787: accuracy:0.94 loss: 27.3971\n",
      "1788: accuracy:0.96 loss: 32.7103\n",
      "1789: accuracy:0.88 loss: 32.6958\n",
      "1790: accuracy:0.87 loss: 54.1174\n",
      "1791: accuracy:0.93 loss: 28.6807\n",
      "1792: accuracy:0.97 loss: 28.2521\n",
      "1793: accuracy:0.92 loss: 23.1179\n",
      "1794: accuracy:0.91 loss: 20.816\n",
      "1795: accuracy:0.88 loss: 37.3679\n",
      "1796: accuracy:0.91 loss: 36.9283\n",
      "1797: accuracy:0.95 loss: 19.6969\n",
      "1798: accuracy:0.95 loss: 25.2953\n",
      "1799: accuracy:0.95 loss: 19.6686\n",
      "1800: accuracy:0.93 loss: 23.2507\n",
      "1801: accuracy:0.89 loss: 56.1711\n",
      "1802: accuracy:0.89 loss: 35.4208\n",
      "1803: accuracy:0.94 loss: 24.9981\n",
      "1804: accuracy:0.92 loss: 36.0456\n",
      "1805: accuracy:0.94 loss: 16.8522\n",
      "1806: accuracy:0.91 loss: 36.3794\n",
      "1807: accuracy:0.91 loss: 25.827\n",
      "1808: accuracy:0.9 loss: 29.9731\n",
      "1809: accuracy:0.9 loss: 32.4521\n",
      "1810: accuracy:0.97 loss: 15.6021\n",
      "1811: accuracy:0.94 loss: 22.862\n",
      "1812: accuracy:0.91 loss: 21.7431\n",
      "1813: accuracy:0.94 loss: 13.8138\n",
      "1814: accuracy:0.85 loss: 43.3482\n",
      "1815: accuracy:0.95 loss: 26.2491\n",
      "1816: accuracy:0.89 loss: 42.165\n",
      "1817: accuracy:0.87 loss: 30.9992\n",
      "1818: accuracy:0.91 loss: 26.7611\n",
      "1819: accuracy:0.95 loss: 16.9204\n",
      "1820: accuracy:0.95 loss: 18.4585\n",
      "1821: accuracy:0.97 loss: 14.639\n",
      "1822: accuracy:0.89 loss: 35.4022\n",
      "1823: accuracy:0.96 loss: 18.3568\n",
      "1824: accuracy:0.89 loss: 41.7493\n",
      "1825: accuracy:0.89 loss: 51.2037\n",
      "1826: accuracy:0.94 loss: 30.169\n",
      "1827: accuracy:0.94 loss: 21.8539\n",
      "1828: accuracy:0.88 loss: 34.442\n",
      "1829: accuracy:0.95 loss: 22.2027\n",
      "1830: accuracy:0.82 loss: 58.0074\n",
      "1831: accuracy:0.93 loss: 26.1324\n",
      "1832: accuracy:0.9 loss: 27.3362\n",
      "1833: accuracy:0.96 loss: 26.6128\n",
      "1834: accuracy:0.93 loss: 16.1682\n",
      "1835: accuracy:0.96 loss: 16.5599\n",
      "1836: accuracy:0.93 loss: 22.973\n",
      "1837: accuracy:0.91 loss: 34.8385\n",
      "1838: accuracy:0.91 loss: 37.1742\n",
      "1839: accuracy:0.9 loss: 34.072\n",
      "1840: accuracy:0.97 loss: 18.4892\n",
      "1841: accuracy:0.93 loss: 28.0483\n",
      "1842: accuracy:0.91 loss: 29.3117\n",
      "1843: accuracy:0.93 loss: 21.7996\n",
      "1844: accuracy:0.89 loss: 33.7494\n",
      "1845: accuracy:0.9 loss: 28.7348\n",
      "1846: accuracy:0.95 loss: 21.5886\n",
      "1847: accuracy:0.9 loss: 29.2405\n",
      "1848: accuracy:0.91 loss: 25.9155\n",
      "1849: accuracy:0.89 loss: 35.2853\n",
      "1850: accuracy:0.93 loss: 32.0981\n",
      "1851: accuracy:0.88 loss: 50.6528\n",
      "1852: accuracy:0.9 loss: 39.4048\n",
      "1853: accuracy:0.92 loss: 22.2209\n",
      "1854: accuracy:0.94 loss: 24.513\n",
      "1855: accuracy:0.95 loss: 24.5357\n",
      "1856: accuracy:0.92 loss: 24.3917\n",
      "1857: accuracy:0.96 loss: 19.9203\n",
      "1858: accuracy:0.95 loss: 19.5293\n",
      "1859: accuracy:0.9 loss: 29.8737\n",
      "1860: accuracy:0.92 loss: 26.5411\n",
      "1861: accuracy:0.95 loss: 26.1436\n",
      "1862: accuracy:0.89 loss: 38.0035\n",
      "1863: accuracy:0.93 loss: 25.0173\n",
      "1864: accuracy:0.91 loss: 29.0831\n",
      "1865: accuracy:0.96 loss: 19.4744\n",
      "1866: accuracy:0.92 loss: 46.8966\n",
      "1867: accuracy:0.95 loss: 20.0447\n",
      "1868: accuracy:0.94 loss: 29.5571\n",
      "1869: accuracy:0.95 loss: 15.1015\n",
      "1870: accuracy:0.91 loss: 21.568\n",
      "1871: accuracy:0.91 loss: 44.6387\n",
      "1872: accuracy:0.93 loss: 24.5953\n",
      "1873: accuracy:0.97 loss: 13.5915\n",
      "1874: accuracy:0.92 loss: 27.1481\n",
      "1875: accuracy:0.95 loss: 17.6276\n",
      "1876: accuracy:0.86 loss: 37.9505\n",
      "1877: accuracy:0.97 loss: 23.5125\n",
      "1878: accuracy:0.95 loss: 19.8142\n",
      "1879: accuracy:0.89 loss: 36.5601\n",
      "1880: accuracy:0.91 loss: 35.6146\n",
      "1881: accuracy:0.93 loss: 29.9971\n",
      "1882: accuracy:0.98 loss: 16.6492\n",
      "1883: accuracy:0.97 loss: 14.2382\n",
      "1884: accuracy:0.9 loss: 31.3281\n",
      "1885: accuracy:0.93 loss: 25.2674\n",
      "1886: accuracy:0.95 loss: 16.1257\n",
      "1887: accuracy:0.94 loss: 25.5301\n",
      "1888: accuracy:0.93 loss: 23.9094\n",
      "1889: accuracy:0.9 loss: 23.62\n",
      "1890: accuracy:0.93 loss: 25.3172\n",
      "1891: accuracy:0.92 loss: 26.0313\n",
      "1892: accuracy:0.93 loss: 19.2515\n",
      "1893: accuracy:0.89 loss: 39.8932\n",
      "1894: accuracy:0.91 loss: 22.9623\n",
      "1895: accuracy:0.92 loss: 22.1606\n",
      "1896: accuracy:0.87 loss: 53.2616\n",
      "1897: accuracy:0.95 loss: 19.3189\n",
      "1898: accuracy:0.95 loss: 30.3973\n",
      "1899: accuracy:0.91 loss: 45.245\n",
      "1900: accuracy:0.98 loss: 12.7904\n",
      "1901: accuracy:0.89 loss: 37.0112\n",
      "1902: accuracy:0.96 loss: 18.3024\n",
      "1903: accuracy:0.88 loss: 31.662\n",
      "1904: accuracy:0.93 loss: 20.0602\n",
      "1905: accuracy:0.91 loss: 28.1298\n",
      "1906: accuracy:0.93 loss: 32.0632\n",
      "1907: accuracy:0.95 loss: 22.3653\n",
      "1908: accuracy:0.89 loss: 35.6862\n",
      "1909: accuracy:0.93 loss: 24.4535\n",
      "1910: accuracy:0.93 loss: 32.8297\n",
      "1911: accuracy:0.92 loss: 29.629\n",
      "1912: accuracy:0.87 loss: 48.4429\n",
      "1913: accuracy:0.87 loss: 35.9479\n",
      "1914: accuracy:0.96 loss: 11.4915\n",
      "1915: accuracy:0.95 loss: 36.1571\n",
      "1916: accuracy:0.92 loss: 22.2857\n",
      "1917: accuracy:0.9 loss: 28.9212\n",
      "1918: accuracy:0.92 loss: 24.4696\n",
      "1919: accuracy:0.9 loss: 35.2271\n",
      "1920: accuracy:0.92 loss: 30.959\n",
      "1921: accuracy:0.9 loss: 31.1547\n",
      "1922: accuracy:0.92 loss: 35.5213\n",
      "1923: accuracy:0.87 loss: 34.0644\n",
      "1924: accuracy:0.88 loss: 38.8482\n",
      "1925: accuracy:0.93 loss: 20.9398\n",
      "1926: accuracy:0.87 loss: 34.5509\n",
      "1927: accuracy:0.93 loss: 25.0419\n",
      "1928: accuracy:0.85 loss: 42.0001\n",
      "1929: accuracy:0.91 loss: 25.552\n",
      "1930: accuracy:0.92 loss: 35.4356\n",
      "1931: accuracy:0.93 loss: 28.8306\n",
      "1932: accuracy:0.94 loss: 28.4974\n",
      "1933: accuracy:0.91 loss: 28.0545\n",
      "1934: accuracy:0.92 loss: 20.3371\n",
      "1935: accuracy:0.92 loss: 34.7796\n",
      "1936: accuracy:0.89 loss: 37.9821\n",
      "1937: accuracy:0.92 loss: 30.0967\n",
      "1938: accuracy:0.92 loss: 31.2351\n",
      "1939: accuracy:0.88 loss: 43.476\n",
      "1940: accuracy:0.91 loss: 27.9724\n",
      "1941: accuracy:0.9 loss: 33.7907\n",
      "1942: accuracy:0.9 loss: 29.4915\n",
      "1943: accuracy:0.93 loss: 35.5209\n",
      "1944: accuracy:0.93 loss: 23.1822\n",
      "1945: accuracy:0.92 loss: 23.0475\n",
      "1946: accuracy:0.92 loss: 28.8083\n",
      "1947: accuracy:0.88 loss: 45.9824\n",
      "1948: accuracy:0.96 loss: 18.6631\n",
      "1949: accuracy:0.89 loss: 31.2672\n",
      "1950: accuracy:0.94 loss: 22.6609\n",
      "1951: accuracy:0.87 loss: 37.3535\n",
      "1952: accuracy:0.95 loss: 18.7723\n",
      "1953: accuracy:0.91 loss: 44.5868\n",
      "1954: accuracy:0.93 loss: 26.6176\n",
      "1955: accuracy:0.95 loss: 25.2334\n",
      "1956: accuracy:0.96 loss: 16.9432\n",
      "1957: accuracy:0.88 loss: 46.3797\n",
      "1958: accuracy:0.91 loss: 29.1556\n",
      "1959: accuracy:0.96 loss: 21.1872\n",
      "1960: accuracy:0.93 loss: 21.9754\n",
      "1961: accuracy:0.94 loss: 33.5696\n",
      "1962: accuracy:0.91 loss: 27.3161\n",
      "1963: accuracy:0.92 loss: 26.0153\n",
      "1964: accuracy:0.89 loss: 38.1246\n",
      "1965: accuracy:0.93 loss: 21.2971\n",
      "1966: accuracy:0.95 loss: 16.5234\n",
      "1967: accuracy:0.98 loss: 7.83018\n",
      "1968: accuracy:0.92 loss: 33.4872\n",
      "1969: accuracy:0.99 loss: 7.1261\n",
      "1970: accuracy:0.94 loss: 24.7458\n",
      "1971: accuracy:0.86 loss: 33.0907\n",
      "1972: accuracy:0.88 loss: 30.2735\n",
      "1973: accuracy:0.86 loss: 36.9628\n",
      "1974: accuracy:0.92 loss: 37.8981\n",
      "1975: accuracy:0.88 loss: 38.0525\n",
      "1976: accuracy:0.96 loss: 18.9517\n",
      "1977: accuracy:0.91 loss: 24.6751\n",
      "1978: accuracy:0.93 loss: 20.149\n",
      "1979: accuracy:0.96 loss: 18.0944\n",
      "1980: accuracy:0.9 loss: 32.1017\n",
      "1981: accuracy:0.95 loss: 37.3853\n",
      "1982: accuracy:0.95 loss: 13.4408\n",
      "1983: accuracy:0.94 loss: 20.8235\n",
      "1984: accuracy:0.89 loss: 39.9101\n",
      "1985: accuracy:0.91 loss: 34.6887\n",
      "1986: accuracy:0.9 loss: 39.158\n",
      "1987: accuracy:0.94 loss: 27.8671\n",
      "1988: accuracy:0.91 loss: 31.1069\n",
      "1989: accuracy:0.94 loss: 19.1076\n",
      "1990: accuracy:0.93 loss: 32.6042\n",
      "1991: accuracy:0.92 loss: 28.5252\n",
      "1992: accuracy:0.91 loss: 33.2765\n",
      "1993: accuracy:0.96 loss: 13.6774\n",
      "1994: accuracy:0.91 loss: 34.1261\n",
      "1995: accuracy:0.97 loss: 16.8849\n",
      "1996: accuracy:0.9 loss: 40.3384\n",
      "1997: accuracy:0.89 loss: 35.5977\n",
      "1998: accuracy:0.84 loss: 33.9228\n",
      "1999: accuracy:0.87 loss: 48.8289\n",
      "2000: accuracy:0.93 loss: 31.1793\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000+1): training_step(i, i % 50 == 0, i % 10 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datavis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-b014c24a62bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max test accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatavis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datavis' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"max test accuracy: \" + str(datavis.get_max_test_accuracy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
